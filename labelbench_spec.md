# LabelBench: Technical Specification & Implementation Guide

> **Tool Compatibility:** This spec is optimized for both Claude Code (agentic terminal execution) and Cursor (IDE-integrated development).
> 
> **For Claude Code:** Follow sections sequentially. All code blocks are complete and ready to execute.
> 
> **For Cursor:** Use this as reference. Architecture and rationale sections help with inline suggestions.

## Project Overview

**LabelBench** is a lightweight Python-based web application for annotating LLM responses to build golden evaluation datasets. It provides a streamlined interface for capturing binary quality decisions (Accept/Reject) with structured feedback, stores annotations locally in SQLite, and offers an interactive dashboard focused on error analysis.

**Tech Stack:** Python 3.10+, Streamlit, SQLite, Pandas, Pydantic, Plotly  
**Package Manager:** uv (modern Python package management)

---

## Quick Start (For AI Coding Tools)

### Initial Setup Commands

```bash
# Create project directory
mkdir labelbench
cd labelbench

# Initialize project with uv
uv init

# Initialize git repository
git init
git branch -m main

# Create .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
.venv/
venv/
env/
*.egg-info/

# uv
.python-version

# Database
*.db
labelbench.db

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Streamlit
.streamlit/secrets.toml
EOF

# Initial commit
git add .gitignore
git commit -m "Initial commit: Add .gitignore"
```

---

## Core Problem Being Solved

ML practitioners building LLM applications need to manually validate model outputs before deploying automated evaluation. Current solutions are either too heavyweight (enterprise annotation platforms) or too unstructured (spreadsheets). LabelBench makes it painless to:

1. Import prompt-response pairs from existing logging systems
2. Systematically annotate quality with structured feedback
3. Perform error analysis to identify failure patterns
4. Export rejected samples for downstream evaluation work

**Critical insight:** Error analysis is the most painful part of the workflow, so the tool prioritizes making this as seamless as possible.

---

## User Workflow

```
1. Import Dataset
   ↓ User uploads CSV/JSON with prompt-response pairs
   
2. Annotate Samples
   ↓ Review each sample, make binary Accept/Reject decision
   ↓ If rejected: select primary issue type + add notes
   
3. Error Analysis (THE MAIN VALUE)
   ↓ Interactive dashboard showing error distribution
   ↓ Click error type → see all examples with that issue
   ↓ Read notes to understand patterns
   
4. Export Results
   ↓ Download rejected samples for fixing
   ↓ Generate summary report documenting findings
```

---

### Phase 1: Dependencies & Setup (PR #1)

#### Step 1.1: Create pyproject.toml with uv

```bash
# uv will create pyproject.toml automatically, but we'll specify our dependencies
cat > pyproject.toml << 'EOF'
[project]
name = "labelbench"
version = "0.1.0"
description = "Lightweight annotation tool for LLM evaluation datasets"
requires-python = ">=3.10"
dependencies = [
    "streamlit>=1.31.0",
    "pandas>=2.0.0",
    "plotly>=5.18.0",
    "pydantic>=2.5.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
]
EOF
```

#### Step 1.2: Install dependencies with uv

```bash
# Install all dependencies
uv sync

# Install with dev dependencies
uv sync --extra dev
```

#### Step 1.3: Commit

```bash
git add pyproject.toml
git commit -m "feat: Add project dependencies via pyproject.toml"
```

---

### Phase 2: Data Models (PR #1)

#### Step 2.1: Create models/sample.py

**File: `models/sample.py`**

```python
"""Sample data model for prompt-response pairs."""

from pydantic import BaseModel, Field
from datetime import datetime, timezone
from typing import Dict, Any


class Sample(BaseModel):
    """Represents a prompt-response pair to be annotated.
    
    Attributes:
        id: Unique identifier for the sample
        prompt: Input text sent to the LLM
        response: Output text generated by the LLM
        metadata: Flexible dictionary for additional context (model, task_type, etc.)
        imported_at: Timestamp when sample was imported into the system
    """
    id: str = Field(..., description="Unique identifier for the sample")
    prompt: str = Field(..., description="Input to the LLM")
    response: str = Field(..., description="LLM's output")
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Flexible metadata (model, task_type, etc.)"
    )
    imported_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "sample_001",
                "prompt": "What is the capital of France?",
                "response": "The capital of France is Paris.",
                "metadata": {
                    "model": "gpt-4",
                    "task_type": "factual_qa"
                }
            }
        }
```

**Command to create:**

```bash
cat > models/sample.py << 'EOF'
[paste the complete code above]
EOF
```

#### Step 2.2: Create models/annotation.py

**File: `models/annotation.py`**

```python
"""Annotation data model for human feedback."""

from pydantic import BaseModel, Field
from datetime import datetime, timezone
from typing import Optional, Literal
import uuid


class Annotation(BaseModel):
    """Represents human feedback on a sample.
    
    Attributes:
        id: Unique identifier for the annotation
        sample_id: Foreign key reference to the Sample being annotated
        annotator_id: Identifier for the person annotating (default for single-user)
        is_acceptable: Binary quality decision (True = Accept, False = Reject)
        primary_issue: Main reason for rejection (only set if is_acceptable=False)
        notes: Free-form text explanation or additional context
        annotated_at: Timestamp when annotation was created
    """
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    sample_id: str = Field(..., description="Foreign key to Sample")
    annotator_id: str = Field(
        default="default",
        description="For future multi-annotator support"
    )
    
    # Core judgment
    is_acceptable: bool = Field(..., description="Binary quality decision")
    
    # Structured feedback (only required if rejected)
    primary_issue: Optional[Literal[
        "hallucination",
        "factually_incorrect",
        "incomplete",
        "wrong_format",
        "off_topic",
        "inappropriate_tone",
        "refusal",
        "other"
    ]] = Field(None, description="Main reason for rejection")
    
    # Free-form notes
    notes: Optional[str] = Field(None, description="Detailed explanation")
    
    # Metadata
    annotated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    
    class Config:
        json_schema_extra = {
            "example": {
                "sample_id": "sample_001",
                "is_acceptable": False,
                "primary_issue": "hallucination",
                "notes": "The response claims free shipping to Alaska, but we charge $15."
            }
        }
```

**Command to create:**

```bash
cat > models/annotation.py << 'EOF'
[paste the complete code above]
EOF
```

#### Step 2.3: Test models

**File: `tests/test_models.py`**

```python
"""Unit tests for data models."""

import pytest
from models.sample import Sample
from models.annotation import Annotation


def test_sample_creation():
    """Test creating a Sample with required fields."""
    sample = Sample(
        id="test_1",
        prompt="What is 2+2?",
        response="4",
        metadata={"model": "gpt-4"}
    )
    
    assert sample.id == "test_1"
    assert sample.prompt == "What is 2+2?"
    assert sample.metadata["model"] == "gpt-4"


def test_sample_without_metadata():
    """Test creating a Sample without metadata."""
    sample = Sample(
        id="test_2",
        prompt="Test prompt",
        response="Test response"
    )
    
    assert sample.metadata == {}


def test_annotation_accepted():
    """Test creating an accepted annotation."""
    annotation = Annotation(
        sample_id="test_1",
        is_acceptable=True
    )
    
    assert annotation.is_acceptable is True
    assert annotation.primary_issue is None
    assert annotation.notes is None


def test_annotation_rejected():
    """Test creating a rejected annotation with feedback."""
    annotation = Annotation(
        sample_id="test_1",
        is_acceptable=False,
        primary_issue="hallucination",
        notes="Incorrect information provided"
    )
    
    assert annotation.is_acceptable is False
    assert annotation.primary_issue == "hallucination"
    assert annotation.notes == "Incorrect information provided"


def test_annotation_auto_id():
    """Test that annotations get auto-generated IDs."""
    ann1 = Annotation(sample_id="test_1", is_acceptable=True)
    ann2 = Annotation(sample_id="test_1", is_acceptable=True)
    
    assert ann1.id != ann2.id
    assert len(ann1.id) == 36  # UUID4 format
```

**Command to create:**

```bash
cat > tests/test_models.py << 'EOF'
[paste the complete code above]
EOF
```

**Run tests:**

```bash
uv run pytest tests/test_models.py -v
```

#### Step 2.4: Commit data models

```bash
git add models/ tests/test_models.py
git commit -m "feat: Add Sample and Annotation Pydantic models with tests"
```

---

### Phase 3: Database Layer (PR #1)

#### Step 3.1: Create storage/database.py

**File: `storage/database.py`**

```python
"""Database operations for LabelBench using SQLite."""

import sqlite3
import json
import logging
from contextlib import contextmanager
from typing import List, Optional, Dict, Any, Tuple
from pathlib import Path

from models.sample import Sample
from models.annotation import Annotation

logger = logging.getLogger(__name__)


class Database:
    """SQLite database handler for samples and annotations.
    
    This class provides CRUD operations for storing and retrieving
    prompt-response samples and their annotations.
    """
    
    def __init__(self, db_path: str = "labelbench.db"):
        """Initialize database connection and create tables if needed.
        
        Args:
            db_path: Path to SQLite database file
        """
        self.db_path = db_path
        self._init_db()
    
    @contextmanager
    def _get_connection(self):
        """Get a database connection with automatic transaction management.
        
        Yields:
            SQLite connection object
            
        Usage:
            with self._get_connection() as conn:
                conn.execute(...)
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()
    
    def _parse_metadata(self, metadata_str: str) -> Dict[str, Any]:
        """Safely parse metadata JSON string.
        
        Args:
            metadata_str: JSON string or None
            
        Returns:
            Parsed metadata dictionary, empty dict if invalid or empty
        """
        if not metadata_str or metadata_str.strip() == '':
            return {}
        try:
            return json.loads(metadata_str)
        except (json.JSONDecodeError, TypeError) as e:
            logger.warning(f"Invalid metadata JSON: {metadata_str[:100]}... Error: {e}")
            return {}
    
    def _init_db(self):
        """Create tables if they don't exist."""
        with self._get_connection() as conn:
            # Create samples table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS samples (
                    id TEXT PRIMARY KEY,
                    prompt TEXT NOT NULL,
                    response TEXT NOT NULL,
                    metadata TEXT,
                    imported_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_samples_imported 
                ON samples(imported_at)
            """)
            
            # Create annotations table
            conn.execute("""
                CREATE TABLE IF NOT EXISTS annotations (
                    id TEXT PRIMARY KEY,
                    sample_id TEXT NOT NULL,
                    annotator_id TEXT DEFAULT 'default',
                    is_acceptable BOOLEAN NOT NULL,
                    primary_issue TEXT,
                    notes TEXT,
                    annotated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (sample_id) REFERENCES samples(id) ON DELETE CASCADE
                )
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_annotations_sample 
                ON annotations(sample_id)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_annotations_acceptable 
                ON annotations(is_acceptable)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_annotations_issue 
                ON annotations(primary_issue)
            """)
    
    def insert_samples(self, samples: List[Sample]) -> int:
        """Insert samples into database, skipping duplicates.
        
        Args:
            samples: List of Sample objects to insert
            
        Returns:
            Number of samples successfully inserted
        """
        inserted = 0
        with self._get_connection() as conn:
            for sample in samples:
                try:
                    conn.execute(
                        """INSERT INTO samples (id, prompt, response, metadata, imported_at) 
                           VALUES (?, ?, ?, ?, ?)""",
                        (
                            sample.id,
                            sample.prompt,
                            sample.response,
                            json.dumps(sample.metadata),
                            sample.imported_at
                        )
                    )
                    inserted += 1
                except sqlite3.IntegrityError:
                    # Skip duplicate IDs
                    continue
        return inserted
    
    def get_sample(self, sample_id: str) -> Optional[Sample]:
        """Retrieve a single sample by ID.
        
        Args:
            sample_id: Unique identifier of the sample
            
        Returns:
            Sample object if found, None otherwise
        """
        with self._get_connection() as conn:
            cursor = conn.execute(
                "SELECT * FROM samples WHERE id = ?",
                (sample_id,)
            )
            row = cursor.fetchone()
        
        if row:
            return Sample(
                id=row['id'],
                prompt=row['prompt'],
                response=row['response'],
                metadata=self._parse_metadata(row['metadata']),
                imported_at=row['imported_at']
            )
        return None
    
    def get_all_samples(self) -> List[Sample]:
        """Retrieve all samples from database.
        
        Returns:
            List of all Sample objects
        """
        with self._get_connection() as conn:
            cursor = conn.execute("SELECT * FROM samples ORDER BY imported_at")
            rows = cursor.fetchall()
        
        return [
            Sample(
                id=row['id'],
                prompt=row['prompt'],
                response=row['response'],
                metadata=self._parse_metadata(row['metadata']),
                imported_at=row['imported_at']
            )
            for row in rows
        ]
    
    def get_unannotated_samples(self) -> List[Sample]:
        """Get samples that haven't been annotated yet.
        
        Returns:
            List of Sample objects without annotations
        """
        with self._get_connection() as conn:
            cursor = conn.execute("""
                SELECT s.* FROM samples s
                LEFT JOIN annotations a ON s.id = a.sample_id
                WHERE a.id IS NULL
                ORDER BY s.imported_at
            """)
            rows = cursor.fetchall()
        
        return [
            Sample(
                id=row['id'],
                prompt=row['prompt'],
                response=row['response'],
                metadata=self._parse_metadata(row['metadata']),
                imported_at=row['imported_at']
            )
            for row in rows
        ]
    
    def insert_annotation(self, annotation: Annotation):
        """Save an annotation to the database.
        
        Args:
            annotation: Annotation object to save
        """
        with self._get_connection() as conn:
            conn.execute("""
                INSERT OR REPLACE INTO annotations 
                (id, sample_id, annotator_id, is_acceptable, primary_issue, notes, annotated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                annotation.id,
                annotation.sample_id,
                annotation.annotator_id,
                annotation.is_acceptable,
                annotation.primary_issue,
                annotation.notes,
                annotation.annotated_at
            ))
    
    def get_annotation(self, sample_id: str) -> Optional[Annotation]:
        """Get annotation for a specific sample.
        
        Args:
            sample_id: ID of the sample
            
        Returns:
            Annotation object if exists, None otherwise
        """
        with self._get_connection() as conn:
            cursor = conn.execute(
                "SELECT * FROM annotations WHERE sample_id = ?",
                (sample_id,)
            )
            row = cursor.fetchone()
        
        if row:
            return Annotation(
                id=row['id'],
                sample_id=row['sample_id'],
                annotator_id=row['annotator_id'],
                is_acceptable=bool(row['is_acceptable']),
                primary_issue=row['primary_issue'],
                notes=row['notes'],
                annotated_at=row['annotated_at']
            )
        return None
    
    def get_annotation_stats(self) -> Dict[str, Any]:
        """Get summary statistics about annotations.
        
        Returns:
            Dictionary with keys: total_annotated, accepted, rejected, acceptance_rate
        """
        with self._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_annotated,
                    SUM(CASE WHEN is_acceptable = 1 THEN 1 ELSE 0 END) as accepted,
                    SUM(CASE WHEN is_acceptable = 0 THEN 1 ELSE 0 END) as rejected
                FROM annotations
            """)
            row = cursor.fetchone()
        
        total = row['total_annotated'] or 0
        accepted = row['accepted'] or 0
        rejected = row['rejected'] or 0
        
        return {
            "total_annotated": total,
            "accepted": accepted,
            "rejected": rejected,
            "acceptance_rate": (accepted / total * 100) if total > 0 else 0
        }
    
    def get_error_distribution(self) -> Dict[str, int]:
        """Get count of each error type.
        
        Returns:
            Dictionary mapping primary_issue to count
        """
        with self._get_connection() as conn:
            cursor = conn.execute("""
                SELECT primary_issue, COUNT(*) as count
                FROM annotations
                WHERE is_acceptable = 0 AND primary_issue IS NOT NULL
                GROUP BY primary_issue
                ORDER BY count DESC
            """)
            rows = cursor.fetchall()
        
        return {row['primary_issue']: row['count'] for row in rows}
    
    def get_samples_by_issue(self, issue_type: str) -> List[Tuple[Sample, Annotation]]:
        """Get all samples with a specific issue type.
        
        Args:
            issue_type: The primary_issue value to filter by
            
        Returns:
            List of (Sample, Annotation) tuples
        """
        with self._get_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    s.id, s.prompt, s.response, s.metadata, s.imported_at,
                    a.id as ann_id, a.sample_id, a.annotator_id, a.is_acceptable,
                    a.primary_issue, a.notes, a.annotated_at
                FROM samples s
                JOIN annotations a ON s.id = a.sample_id
                WHERE a.primary_issue = ?
                ORDER BY a.annotated_at DESC
            """, (issue_type,))
            rows = cursor.fetchall()
        
        results = []
        for row in rows:
            sample = Sample(
                id=row['id'],
                prompt=row['prompt'],
                response=row['response'],
                metadata=self._parse_metadata(row['metadata']),
                imported_at=row['imported_at']
            )
            annotation = Annotation(
                id=row['ann_id'],
                sample_id=row['sample_id'],
                annotator_id=row['annotator_id'],
                is_acceptable=bool(row['is_acceptable']),
                primary_issue=row['primary_issue'],
                notes=row['notes'],
                annotated_at=row['annotated_at']
            )
            results.append((sample, annotation))
        
        return results
    
    def get_total_samples(self) -> int:
        """Get total number of samples in database.
        
        Returns:
            Count of samples
        """
        with self._get_connection() as conn:
            cursor = conn.execute("SELECT COUNT(*) as count FROM samples")
            row = cursor.fetchone()
        return row['count']
```

**Command to create** (for Claude Code):

```bash
# Due to length, create file interactively or use heredoc with EOF markers
cat > storage/database.py << 'DBEOF'
[paste the complete code above]
DBEOF
```

#### Step 3.2: Test database operations

**File: `tests/test_database.py`**

```python
"""Unit tests for database operations."""

import pytest
import os
from storage.database import Database
from models.sample import Sample
from models.annotation import Annotation


@pytest.fixture
def test_db():
    """Create a temporary test database."""
    db = Database(":memory:")
    yield db
    # No cleanup needed for in-memory database


def test_insert_and_retrieve_sample(test_db):
    """Test inserting and retrieving a sample."""
    sample = Sample(
        id="test_1",
        prompt="Test prompt",
        response="Test response",
        metadata={"model": "gpt-4"}
    )
    
    inserted = test_db.insert_samples([sample])
    assert inserted == 1
    
    retrieved = test_db.get_sample("test_1")
    assert retrieved is not None
    assert retrieved.prompt == "Test prompt"
    assert retrieved.metadata["model"] == "gpt-4"


def test_skip_duplicate_samples(test_db):
    """Test that duplicate sample IDs are skipped."""
    sample1 = Sample(id="dup_1", prompt="First", response="First response")
    sample2 = Sample(id="dup_1", prompt="Second", response="Second response")
    
    inserted = test_db.insert_samples([sample1, sample2])
    assert inserted == 1  # Only first should be inserted
    
    retrieved = test_db.get_sample("dup_1")
    assert retrieved.prompt == "First"  # First one wins


def test_get_unannotated_samples(test_db):
    """Test retrieving unannotated samples."""
    # Insert 3 samples
    samples = [
        Sample(id=f"s{i}", prompt=f"Prompt {i}", response=f"Response {i}")
        for i in range(3)
    ]
    test_db.insert_samples(samples)
    
    # Annotate only the first one
    annotation = Annotation(sample_id="s0", is_acceptable=True)
    test_db.insert_annotation(annotation)
    
    # Should get 2 unannotated
    unannotated = test_db.get_unannotated_samples()
    assert len(unannotated) == 2
    assert all(s.id in ["s1", "s2"] for s in unannotated)


def test_annotation_stats(test_db):
    """Test annotation statistics calculation."""
    # Insert samples
    samples = [Sample(id=f"s{i}", prompt="p", response="r") for i in range(10)]
    test_db.insert_samples(samples)
    
    # Annotate: 7 accepted, 3 rejected
    for i in range(7):
        test_db.insert_annotation(Annotation(sample_id=f"s{i}", is_acceptable=True))
    for i in range(7, 10):
        test_db.insert_annotation(Annotation(
            sample_id=f"s{i}",
            is_acceptable=False,
            primary_issue="hallucination"
        ))
    
    stats = test_db.get_annotation_stats()
    assert stats['total_annotated'] == 10
    assert stats['accepted'] == 7
    assert stats['rejected'] == 3
    assert stats['acceptance_rate'] == 70.0


def test_error_distribution(test_db):
    """Test error distribution aggregation."""
    # Insert samples
    samples = [Sample(id=f"s{i}", prompt="p", response="r") for i in range(5)]
    test_db.insert_samples(samples)
    
    # Create annotations with different issues
    issues = ["hallucination", "hallucination", "incomplete", "hallucination", "wrong_format"]
    for i, issue in enumerate(issues):
        test_db.insert_annotation(Annotation(
            sample_id=f"s{i}",
            is_acceptable=False,
            primary_issue=issue
        ))
    
    distribution = test_db.get_error_distribution()
    assert distribution["hallucination"] == 3
    assert distribution["incomplete"] == 1
    assert distribution["wrong_format"] == 1


def test_get_samples_by_issue(test_db):
    """Test filtering samples by issue type."""
    # Insert samples
    samples = [Sample(id=f"s{i}", prompt=f"Prompt {i}", response=f"Response {i}") 
               for i in range(3)]
    test_db.insert_samples(samples)
    
    # Annotate with different issues
    test_db.insert_annotation(Annotation(
        sample_id="s0",
        is_acceptable=False,
        primary_issue="hallucination",
        notes="Note 0"
    ))
    test_db.insert_annotation(Annotation(
        sample_id="s1",
        is_acceptable=False,
        primary_issue="hallucination",
        notes="Note 1"
    ))
    test_db.insert_annotation(Annotation(
        sample_id="s2",
        is_acceptable=False,
        primary_issue="incomplete"
    ))
    
    # Get only hallucinations
    results = test_db.get_samples_by_issue("hallucination")
    assert len(results) == 2
    
    # Verify structure
    sample, annotation = results[0]
    assert isinstance(sample, Sample)
    assert isinstance(annotation, Annotation)
    assert annotation.primary_issue == "hallucination"
```

**Command to create:**

```bash
cat > tests/test_database.py << 'TESTEOF'
[paste the complete code above]
TESTEOF
```

**Run tests:**

```bash
uv run pytest tests/test_database.py -v
```

#### Step 3.3: Commit database layer

```bash
git add storage/database.py tests/test_database.py
git commit -m "feat: Add SQLite database layer with CRUD operations"
git push origin main

# Create PR #1
# Title: "Foundation: Data models and database layer"
# Description: "Adds Pydantic models for Sample and Annotation, SQLite database with CRUD operations, and comprehensive unit tests"
```

---

### Phase 4: Import/Export (PR #2)

#### Step 4.1: Create storage/import_export.py

**File: `storage/import_export.py`**

```python
"""Import and export utilities for CSV/JSON data."""

import pandas as pd
import json
import re
from typing import List
from pathlib import Path

from models.sample import Sample


def import_csv(file_path: str) -> List[Sample]:
    """Import samples from CSV file.
    
    Expected columns: id, prompt, response
    Optional columns: Any additional columns treated as metadata
    
    Args:
        file_path: Path to CSV file
        
    Returns:
        List of Sample objects
        
    Raises:
        ValueError: If required columns are missing
        FileNotFoundError: If file doesn't exist
    """
    if not Path(file_path).exists():
        raise FileNotFoundError(f"File not found: {file_path}")
    
    df = pd.read_csv(file_path)
    
    # Validate required columns
    required = ['id', 'prompt', 'response']
    missing = set(required) - set(df.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    
    # Convert to Sample objects with validation
    samples = []
    for idx, row in df.iterrows():
        # Validate required fields are not empty
        sample_id = str(row['id']).strip()
        if not sample_id:
            raise ValueError(f"Row {idx + 1}: 'id' cannot be empty")
        
        prompt = str(row['prompt']).strip()
        if not prompt:
            raise ValueError(f"Row {idx + 1}: 'prompt' cannot be empty")
        
        response = str(row['response']).strip()
        if not response:
            raise ValueError(f"Row {idx + 1}: 'response' cannot be empty")
        
        # Validate ID format (no SQL injection risks or special characters)
        if not re.match(r'^[a-zA-Z0-9_-]+$', sample_id):
            raise ValueError(f"Row {idx + 1}: 'id' contains invalid characters. Only letters, numbers, underscores, and hyphens allowed.")
        
        # Truncate extremely long text (prevent UI crashes)
        if len(prompt) > 10000:
            prompt = prompt[:10000]
        if len(response) > 50000:
            response = response[:50000]
        
        # Separate required fields from metadata
        metadata = {}
        for col in df.columns:
            if col not in required and pd.notna(row[col]):
                metadata[col] = row[col]
        
        sample = Sample(
            id=sample_id,
            prompt=prompt,
            response=response,
            metadata=metadata
        )
        samples.append(sample)
    
    return samples


def import_json(file_path: str) -> List[Sample]:
    """Import samples from JSON file.
    
    Expected format:
    {
        "samples": [
            {
                "id": "...",
                "prompt": "...",
                "response": "...",
                "metadata": {...}  # optional
            }
        ]
    }
    
    Args:
        file_path: Path to JSON file
        
    Returns:
        List of Sample objects
        
    Raises:
        ValueError: If JSON structure is invalid
        FileNotFoundError: If file doesn't exist
    """
    if not Path(file_path).exists():
        raise FileNotFoundError(f"File not found: {file_path}")
    
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    if 'samples' not in data:
        raise ValueError("JSON must contain 'samples' key with array of sample objects")
    
    samples = []
    for i, item in enumerate(data['samples']):
        # Validate required fields exist
        if 'id' not in item:
            raise ValueError(f"Sample at index {i} missing 'id' field")
        if 'prompt' not in item:
            raise ValueError(f"Sample at index {i} missing 'prompt' field")
        if 'response' not in item:
            raise ValueError(f"Sample at index {i} missing 'response' field")
        
        # Validate required fields are not empty
        sample_id = str(item['id']).strip()
        if not sample_id:
            raise ValueError(f"Sample at index {i}: 'id' cannot be empty")
        
        prompt = str(item['prompt']).strip()
        if not prompt:
            raise ValueError(f"Sample at index {i}: 'prompt' cannot be empty")
        
        response = str(item['response']).strip()
        if not response:
            raise ValueError(f"Sample at index {i}: 'response' cannot be empty")
        
        # Validate ID format
        if not re.match(r'^[a-zA-Z0-9_-]+$', sample_id):
            raise ValueError(f"Sample at index {i}: 'id' contains invalid characters. Only letters, numbers, underscores, and hyphens allowed.")
        
        # Truncate extremely long text (prevent UI crashes)
        if len(prompt) > 10000:
            prompt = prompt[:10000]
        if len(response) > 50000:
            response = response[:50000]
        
        sample = Sample(
            id=sample_id,
            prompt=prompt,
            response=response,
            metadata=item.get('metadata', {})
        )
        samples.append(sample)
    
    return samples


def export_rejected_csv(db, output_path: str) -> int:
    """Export all rejected samples with annotations to CSV.
    
    Args:
        db: Database instance
        output_path: Path for output CSV file
        
    Returns:
        Number of samples exported
    """
    with db._get_connection() as conn:
        df = pd.read_sql_query("""
            SELECT 
                s.id,
                s.prompt,
                s.response,
                s.metadata,
                a.primary_issue,
                a.notes,
                a.annotated_at
            FROM samples s
            JOIN annotations a ON s.id = a.sample_id
            WHERE a.is_acceptable = 0
            ORDER BY a.annotated_at DESC
        """, conn)
    
    if df.empty:
        # Create empty file
        df.to_csv(output_path, index=False)
        return 0
    
    # Parse metadata into separate columns with error handling
    def safe_json_loads(x):
        if not x or x.strip() == '':
            return {}
        try:
            return json.loads(x)
        except (json.JSONDecodeError, TypeError):
            return {}
    
    metadata_df = df['metadata'].apply(safe_json_loads).apply(pd.Series)
    
    # Combine with main dataframe
    df = pd.concat([
        df.drop('metadata', axis=1),
        metadata_df
    ], axis=1)
    
    df.to_csv(output_path, index=False)
    return len(df)


def export_all_annotations_json(db, output_path: str) -> int:
    """Export all samples with annotations to JSON.
    
    Args:
        db: Database instance
        output_path: Path for output JSON file
        
    Returns:
        Number of samples exported
    """
    with db._get_connection() as conn:
        df = pd.read_sql_query("""
            SELECT 
                s.id,
                s.prompt,
                s.response,
                s.metadata,
                a.is_acceptable,
                a.primary_issue,
                a.notes,
                a.annotated_at
            FROM samples s
            JOIN annotations a ON s.id = a.sample_id
            ORDER BY s.id
        """, conn)
    
    if df.empty:
        with open(output_path, 'w') as f:
            json.dump({"samples": []}, f, indent=2)
        return 0
    
    # Convert to list of dictionaries
    samples = []
    for _, row in df.iterrows():
        sample = {
            "id": row['id'],
            "prompt": row['prompt'],
            "response": row['response'],
            "metadata": json.loads(row['metadata']) if row['metadata'] else {},
            "annotation": {
                "is_acceptable": bool(row['is_acceptable']),
                "primary_issue": row['primary_issue'],
                "notes": row['notes'],
                "annotated_at": str(row['annotated_at'])
            }
        }
        samples.append(sample)
    
    with open(output_path, 'w') as f:
        json.dump({"samples": samples}, f, indent=2)
    
    return len(samples)
```

**Command to create:**

```bash
cat > storage/import_export.py << 'IOEOF'
[paste the complete code above]
IOEOF
```

#### Step 4.2: Create example dataset

**File: `data/example_samples.csv`**

```csv
id,prompt,response,model,task_type
1,"What's your return policy?","You can return items within 30 days for a full refund with proof of purchase.",gpt-4,customer_support
2,"Do you ship to Canada?","Yes, we offer free shipping to all of Canada!",gpt-3.5,customer_support
3,"What are your business hours?","We're open Monday through Friday, 9 AM to 5 PM EST.",gpt-4,customer_support
4,"Can I return opened products?","All products can be returned regardless of whether they've been opened.",gpt-3.5,customer_support
5,"What's your refund processing time?","Refunds are processed within 30 days of receiving your return.",gpt-4,customer_support
6,"Do you ship internationally?","We ship to over 100 countries worldwide, including free shipping!",gpt-3.5,customer_support
7,"What payment methods do you accept?","We accept all major credit cards, PayPal, and Apple Pay.",gpt-4,customer_support
8,"Is there a warranty on products?","All products come with a comprehensive lifetime warranty.",gpt-3.5,customer_support
9,"How do I track my order?","You'll receive a tracking number via email once your order ships.",gpt-4,customer_support
10,"Can I change my order after placing it?","Yes, you can modify your order anytime before delivery.",gpt-3.5,customer_support
11,"What's your price match policy?","We'll match any competitor's price, no questions asked!",gpt-3.5,customer_support
12,"Do you offer gift wrapping?","Yes, gift wrapping is available for a small fee of $5.",gpt-4,customer_support
13,"How long does shipping take?","Standard shipping takes 3-5 business days within the continental US.",gpt-4,customer_support
14,"Can I cancel my order?","Orders can be cancelled within 24 hours of placement for a full refund.",gpt-4,customer_support
15,"Do you have a loyalty program?","Yes, our rewards program offers 10% back on all purchases.",gpt-4,customer_support
16,"What's your customer service number?","You can reach us at 1-800-HELP-NOW, available 24/7/365.",gpt-3.5,customer_support
17,"Are there any shipping restrictions?","We don't ship to PO boxes or military addresses.",gpt-3.5,customer_support
18,"Can I use multiple discount codes?","Absolutely! Stack as many discount codes as you want.",gpt-3.5,customer_support
19,"What's the minimum order amount?","There's no minimum order amount - order as little as you need.",gpt-4,customer_support
20,"Do you price match after purchase?","Yes, we'll refund the difference if you find a lower price within 60 days.",gpt-3.5,customer_support
```

**Notes on example data (for human context, not in file):**
- Samples 2, 6, 8, 11, 16, 17, 18, 20: Likely hallucinations or incorrect policy statements
- Samples 4, 10: Potentially incomplete or vague
- Samples 1, 3, 5, 7, 9, 12, 13, 14, 15, 19: Generally acceptable responses

**Command to create:**

```bash
cat > data/example_samples.csv << 'CSVEOF'
id,prompt,response,model,task_type
1,"What's your return policy?","You can return items within 30 days for a full refund with proof of purchase.",gpt-4,customer_support
2,"Do you ship to Canada?","Yes, we offer free shipping to all of Canada!",gpt-3.5,customer_support
3,"What are your business hours?","We're open Monday through Friday, 9 AM to 5 PM EST.",gpt-4,customer_support
4,"Can I return opened products?","All products can be returned regardless of whether they've been opened.",gpt-3.5,customer_support
5,"What's your refund processing time?","Refunds are processed within 30 days of receiving your return.",gpt-4,customer_support
6,"Do you ship internationally?","We ship to over 100 countries worldwide, including free shipping!",gpt-3.5,customer_support
7,"What payment methods do you accept?","We accept all major credit cards, PayPal, and Apple Pay.",gpt-4,customer_support
8,"Is there a warranty on products?","All products come with a comprehensive lifetime warranty.",gpt-3.5,customer_support
9,"How do I track my order?","You'll receive a tracking number via email once your order ships.",gpt-4,customer_support
10,"Can I change my order after placing it?","Yes, you can modify your order anytime before delivery.",gpt-3.5,customer_support
11,"What's your price match policy?","We'll match any competitor's price, no questions asked!",gpt-3.5,customer_support
12,"Do you offer gift wrapping?","Yes, gift wrapping is available for a small fee of $5.",gpt-4,customer_support
13,"How long does shipping take?","Standard shipping takes 3-5 business days within the continental US.",gpt-4,customer_support
14,"Can I cancel my order?","Orders can be cancelled within 24 hours of placement for a full refund.",gpt-4,customer_support
15,"Do you have a loyalty program?","Yes, our rewards program offers 10% back on all purchases.",gpt-4,customer_support
16,"What's your customer service number?","You can reach us at 1-800-HELP-NOW, available 24/7/365.",gpt-3.5,customer_support
17,"Are there any shipping restrictions?","We don't ship to PO boxes or military addresses.",gpt-3.5,customer_support
18,"Can I use multiple discount codes?","Absolutely! Stack as many discount codes as you want.",gpt-3.5,customer_support
19,"What's the minimum order amount?","There's no minimum order amount - order as little as you need.",gpt-4,customer_support
20,"Do you price match after purchase?","Yes, we'll refund the difference if you find a lower price within 60 days.",gpt-3.5,customer_support
CSVEOF
```

#### Step 4.3: Test import/export

**File: `tests/test_import.py`**

```python
"""Unit tests for import/export functionality."""

import pytest
import json
import tempfile
from pathlib import Path

from storage.import_export import import_csv, import_json, export_rejected_csv
from storage.database import Database
from models.sample import Sample
from models.annotation import Annotation


def test_import_csv_valid(tmp_path):
    """Test importing a valid CSV file."""
    csv_file = tmp_path / "test.csv"
    csv_file.write_text(
        "id,prompt,response,model,task_type\n"
        "1,Test prompt,Test response,gpt-4,test\n"
        "2,Another prompt,Another response,gpt-3.5,test\n"
    )
    
    samples = import_csv(str(csv_file))
    
    assert len(samples) == 2
    assert samples[0].id == "1"
    assert samples[0].prompt == "Test prompt"
    assert samples[0].metadata["model"] == "gpt-4"
    assert samples[0].metadata["task_type"] == "test"


def test_import_csv_missing_columns(tmp_path):
    """Test that import fails with missing required columns."""
    csv_file = tmp_path / "invalid.csv"
    csv_file.write_text("id,prompt\n1,Test\n")  # Missing 'response'
    
    with pytest.raises(ValueError, match="Missing required columns"):
        import_csv(str(csv_file))


def test_import_json_valid(tmp_path):
    """Test importing a valid JSON file."""
    json_file = tmp_path / "test.json"
    data = {
        "samples": [
            {
                "id": "1",
                "prompt": "Test prompt",
                "response": "Test response",
                "metadata": {"model": "gpt-4"}
            }
        ]
    }
    json_file.write_text(json.dumps(data))
    
    samples = import_json(str(json_file))
    
    assert len(samples) == 1
    assert samples[0].id == "1"
    assert samples[0].prompt == "Test prompt"
    assert samples[0].metadata["model"] == "gpt-4"


def test_import_json_missing_samples_key(tmp_path):
    """Test that import fails without 'samples' key."""
    json_file = tmp_path / "invalid.json"
    json_file.write_text('{"data": []}')
    
    with pytest.raises(ValueError, match="must contain 'samples' key"):
        import_json(str(json_file))


def test_export_rejected_csv(tmp_path):
    """Test exporting rejected samples to CSV."""
    db = Database(":memory:")
    
    # Insert samples
    samples = [
        Sample(id="s1", prompt="p1", response="r1", metadata={"model": "gpt-4"}),
        Sample(id="s2", prompt="p2", response="r2", metadata={"model": "gpt-3.5"})
    ]
    db.insert_samples(samples)
    
    # Annotate one as rejected
    db.insert_annotation(Annotation(
        sample_id="s1",
        is_acceptable=False,
        primary_issue="hallucination",
        notes="Test note"
    ))
    
    # Export
    output_file = tmp_path / "rejected.csv"
    count = export_rejected_csv(db, str(output_file))
    
    assert count == 1
    assert output_file.exists()
    
    # Verify content
    import pandas as pd
    df = pd.read_csv(output_file)
    assert len(df) == 1
    assert df.iloc[0]['id'] == "s1"
    assert df.iloc[0]['primary_issue'] == "hallucination"
    assert df.iloc[0]['model'] == "gpt-4"
```

**Command to create:**

```bash
cat > tests/test_import.py << 'TESTIMPORTEOF'
[paste the complete code above]
TESTIMPORTEOF
```

**Run tests:**

```bash
uv run pytest tests/test_import.py -v
```

#### Step 4.4: Create import UI page

**File: `ui/import_page.py`**

```python
"""Streamlit page for importing data."""

import streamlit as st
import tempfile
from pathlib import Path

from storage.import_export import import_csv, import_json


def show_import_page():
    """Display the import data page."""
    st.title("Import Data")
    
    st.markdown("""
    Upload a CSV or JSON file containing prompt-response pairs to annotate.
    
    **Required fields:** `id`, `prompt`, `response`  
    **Optional fields:** Any additional columns will be stored as metadata (e.g., `model`, `task_type`)
    """)
    
    # File upload
    uploaded_file = st.file_uploader(
        "Choose a file",
        type=['csv', 'json'],
        help="Upload a CSV or JSON file with your samples"
    )
    
    if uploaded_file is not None:
        # Determine file type
        file_extension = Path(uploaded_file.name).suffix.lower()
        
        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            # Import based on file type
            if file_extension == '.csv':
                samples = import_csv(tmp_path)
            elif file_extension == '.json':
                samples = import_json(tmp_path)
            else:
                st.error(f"Unsupported file type: {file_extension}")
                return
            
            st.success(f" Successfully parsed {len(samples)} samples")
            
            # Preview samples
            st.subheader("Preview (first 5 samples)")
            for i, sample in enumerate(samples[:5]):
                with st.expander(f"Sample {i+1}: {sample.id}"):
                    st.text(f"Prompt: {sample.prompt}")
                    st.text(f"Response: {sample.response}")
                    if sample.metadata:
                        st.json(sample.metadata)
            
            # Confirm import
            if st.button("Import to Database", type="primary"):
                db = st.session_state.db
                inserted = db.insert_samples(samples)
                
                if inserted == len(samples):
                    st.success(f" Imported all {inserted} samples successfully!")
                else:
                    st.warning(
                        f" Imported {inserted} out of {len(samples)} samples. "
                        f"{len(samples) - inserted} duplicates were skipped."
                    )
                
                # Clear any cached annotation state
                if 'samples_to_annotate' in st.session_state:
                    del st.session_state.samples_to_annotate
                    del st.session_state.current_index
                
                st.info("Navigate to  Annotate to start annotating samples.")
        
        except Exception as e:
            st.error(f" Error importing file: {str(e)}")
        
        finally:
            # Clean up temporary file
            Path(tmp_path).unlink(missing_ok=True)
    
    # Show current database stats
    st.divider()
    st.subheader("Current Database")
    
    db = st.session_state.db
    total_samples = db.get_total_samples()
    stats = db.get_annotation_stats()
    
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Samples", total_samples)
    col2.metric("Annotated", stats['total_annotated'])
    col3.metric("Remaining", total_samples - stats['total_annotated'])
```

**Command to create:**

```bash
cat > ui/import_page.py << 'IMPORTPAGEEOF'
[paste the complete code above]
IMPORTPAGEEOF
```

#### Step 4.5: Commit import/export

```bash
git add storage/import_export.py tests/test_import.py data/example_samples.csv ui/import_page.py
git commit -m "feat: Add CSV/JSON import/export with example dataset"
git push origin main

# Create PR #2
# Title: "Import/Export: Add data import and example dataset"
# Description: "Implements CSV/JSON import with validation, export functions for rejected samples, example dataset with 20 samples, and Streamlit import UI page"
```

---

### Phase 5: Annotation Interface (PR #3)

#### Step 5.1: Create annotation page

**File: `ui/annotate_page.py`**

```python
"""Streamlit page for annotating samples."""

import streamlit as st

from models.annotation import Annotation


def show_annotate_page():
    """Display the annotation interface."""
    st.title("Annotate Samples")
    
    db = st.session_state.db
    
    # Load unannotated samples
    if 'samples_to_annotate' not in st.session_state:
        st.session_state.samples_to_annotate = db.get_unannotated_samples()
        st.session_state.current_index = 0
    
    samples = st.session_state.samples_to_annotate
    
    # Check if there are samples to annotate
    if not samples:
        st.info(" No samples to annotate. Import data to get started.")
        
        # Show stats
        stats = db.get_annotation_stats()
        if stats['total_annotated'] > 0:
            st.metric("Completed Annotations", stats['total_annotated'])
            st.success("All samples have been annotated!")
        
        return
    
    # Get current sample with bounds checking
    current_idx = st.session_state.get('current_index', 0)
    
    # Validate bounds
    if current_idx < 0:
        current_idx = 0
        st.session_state.current_index = 0
    elif current_idx >= len(samples):
        current_idx = max(0, len(samples) - 1)
        st.session_state.current_index = current_idx
    
    if not samples or current_idx >= len(samples):
        st.warning("No samples available. Please import data first.")
        return
    
    sample = samples[current_idx]
    
    # Progress indicator
    progress = (current_idx + 1) / len(samples)
    st.progress(progress)
    st.caption(f"Sample {current_idx + 1} of {len(samples)} | ID: {sample.id}")
    
    # Display metadata
    if sample.metadata:
        metadata_cols = st.columns(len(sample.metadata))
        for i, (key, value) in enumerate(sample.metadata.items()):
            metadata_cols[i].metric(key, value)
    
    st.divider()
    
    # Display prompt
    st.subheader("Prompt")
    st.text_area(
        "Prompt",
        value=sample.prompt,
        height=100,
        disabled=True,
        label_visibility="collapsed",
        key="prompt_display"
    )
    
    # Display response
    st.subheader("Response")
    st.text_area(
        "Response",
        value=sample.response,
        height=150,
        disabled=True,
        label_visibility="collapsed",
        key="response_display"
    )
    
    st.divider()
    
    # Annotation form
    st.subheader("Quality Assessment")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Accept", use_container_width=True, type="primary"):
            # Save acceptance annotation
            annotation = Annotation(
                sample_id=sample.id,
                is_acceptable=True
            )
            db.insert_annotation(annotation)
            
            # Move to next sample
            _move_to_next_sample(samples)
    
    with col2:
        if st.button("Reject", use_container_width=True):
            st.session_state.show_rejection_form = True
            st.rerun()
    
    # Show rejection form if reject was clicked
    if st.session_state.get('show_rejection_form', False):
        st.divider()
        st.subheader("Rejection Details")
        
        with st.form("rejection_form"):
            primary_issue = st.selectbox(
                "Primary Issue",
                [
                    "hallucination",
                    "factually_incorrect",
                    "incomplete",
                    "wrong_format",
                    "off_topic",
                    "inappropriate_tone",
                    "refusal",
                    "other"
                ],
                help="Select the main reason for rejecting this response"
            )
            
            notes = st.text_area(
                "Notes (optional)",
                placeholder="Provide additional context about why this response was rejected...",
                height=100
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                submitted = st.form_submit_button("Submit Annotation", type="primary", use_container_width=True)
            
            with col2:
                cancelled = st.form_submit_button("Cancel", use_container_width=True)
            
            if submitted:
                # Save rejection annotation
                annotation = Annotation(
                    sample_id=sample.id,
                    is_acceptable=False,
                    primary_issue=primary_issue,
                    notes=notes if notes.strip() else None
                )
                db.insert_annotation(annotation)
                
                # Clear form state
                st.session_state.show_rejection_form = False
                
                # Move to next sample
                _move_to_next_sample(samples)
            
            if cancelled:
                st.session_state.show_rejection_form = False
                st.rerun()
    
    # Navigation
    st.divider()
    
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col1:
        if st.button("← Previous", disabled=(current_idx == 0)):
            st.session_state.current_index -= 1
            st.session_state.show_rejection_form = False
            st.rerun()
    
    with col2:
        jump_to = st.number_input(
            "Jump to sample",
            min_value=1,
            max_value=len(samples),
            value=current_idx + 1,
            key="jump_input"
        )
        if st.button("Go", use_container_width=True):
            # Validate jump_to index
            target_idx = max(0, min(jump_to - 1, len(samples) - 1))
            st.session_state.current_index = target_idx
            st.session_state.show_rejection_form = False
            st.rerun()
    
    with col3:
        if st.button("Next →", disabled=(current_idx == len(samples) - 1)):
            st.session_state.current_index += 1
            st.session_state.show_rejection_form = False
            st.rerun()


def _move_to_next_sample(samples):
    """Helper to move to next sample and handle end of list."""
    current_idx = st.session_state.get('current_index', 0)
    
    # Validate bounds
    if not samples:
        return
    
    if current_idx < 0:
        current_idx = 0
    elif current_idx >= len(samples):
        current_idx = max(0, len(samples) - 1)
    
    if current_idx < len(samples) - 1:
        st.session_state.current_index = current_idx + 1
    else:
        # Reached end, reload unannotated samples
        db = st.session_state.db
        new_samples = db.get_unannotated_samples()
        if new_samples:
            st.session_state.samples_to_annotate = new_samples
            st.session_state.current_index = 0
        else:
            # No more samples, stay at last position
            st.session_state.current_index = len(samples) - 1
    
    st.rerun()
```

**Command to create:**

```bash
cat > ui/annotate_page.py << 'ANNOTATEPAGEEOF'
[paste the complete code above]
ANNOTATEPAGEEOF
```

#### Step 5.2: Create main app entry point

**File: `app.py`**

```python
"""LabelBench - Main Streamlit application."""

import streamlit as st

from storage.database import Database
from ui.annotate_page import show_annotate_page
from ui.import_page import show_import_page


# Page configuration
st.set_page_config(
    page_title="LabelBench",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize database in session state
if 'db' not in st.session_state:
    st.session_state.db = Database()

# Sidebar navigation
with st.sidebar:
    st.title("LabelBench")
    st.caption("Annotation tool for LLM evaluation datasets")
    
    st.divider()
    
    page = st.radio(
        "Navigation",
        ["Annotate", " Error Analysis", " Import Data"],
        label_visibility="collapsed"
    )
    
    st.divider()
    
    # Quick stats in sidebar
    db = st.session_state.db
    total_samples = db.get_total_samples()
    stats = db.get_annotation_stats()
    
    st.metric("Total Samples", total_samples)
    st.metric("Annotated", stats['total_annotated'])
    
    if stats['total_annotated'] > 0:
        st.metric("Acceptance Rate", f"{stats['acceptance_rate']:.1f}%")

# Route to appropriate page
if page == " Annotate":
    show_annotate_page()
elif page == " Error Analysis":
    st.title("Error Analysis")
    st.info("Error analysis dashboard coming in PR #4!")
    st.caption("This page will show interactive charts and breakdowns of annotation data.")
elif page == " Import Data":
    show_import_page()
```

**Command to create:**

```bash
cat > app.py << 'APPEOF'
[paste the complete code above]
APPEOF
```

#### Step 5.3: Test the annotation interface

```bash
# Run the app
uv run streamlit run app.py

# Manual testing checklist:
# 1. Import data/example_samples.csv
# 2. Navigate to Annotate page
# 3. Accept a sample
# 4. Reject a sample with primary issue and notes
# 5. Test Previous/Next navigation
# 6. Test Jump to sample
# 7. Verify progress indicator updates
# 8. Check that accepted/rejected samples are saved to database
```

#### Step 5.4: Commit annotation interface

```bash
git add ui/annotate_page.py app.py
git commit -m "feat: Add annotation interface with binary accept/reject"
git push origin main

# Create PR #3
# Title: "Annotation Interface: Streamlit UI for reviewing samples"
# Description: "Implements annotation page with binary Accept/Reject decisions, conditional feedback form for rejections, navigation controls, and progress tracking. Includes main app.py entry point with sidebar navigation."
```

---

### Phase 6: Error Analysis Dashboard (PR #4)

#### Step 6.1: Create analysis page

**File: `ui/analysis_page.py`**

```python
"""Streamlit page for error analysis dashboard."""

import streamlit as st
import plotly.express as px
import pandas as pd
import json


def show_analysis_page():
    """Display the error analysis dashboard."""
    st.title("Error Analysis")
    
    db = st.session_state.db
    stats = db.get_annotation_stats()
    
    # Overview metrics
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Annotated", stats['total_annotated'])
    col2.metric("Acceptance Rate", f"{stats['acceptance_rate']:.1f}%")
    col3.metric("Rejected", stats['rejected'])
    
    # Check if there are any annotations
    if stats['total_annotated'] == 0:
        st.info("No annotations yet. Start annotating samples to see error analysis.")
        return
    
    # Check if there are any rejections
    if stats['rejected'] == 0:
        st.success(" All annotated samples were accepted! No errors to analyze.")
        return
    
    st.divider()
    
    # Error distribution section
    st.subheader("What's Breaking?")
    st.caption("Click on a bar to see all samples with that issue")
    
    error_dist = db.get_error_distribution()
    
    if not error_dist:
        st.warning("No error types captured yet. Make sure to select a primary issue when rejecting samples.")
        return
    
    # Create dataframe for plotting
    df_errors = pd.DataFrame({
        'Issue Type': list(error_dist.keys()),
        'Count': list(error_dist.values())
    })
    
    # Calculate percentages
    total_errors = df_errors['Count'].sum()
    df_errors['Percentage'] = (df_errors['Count'] / total_errors * 100).round(1)
    df_errors = df_errors.sort_values('Count', ascending=False)
    
    # Create interactive Plotly chart
    fig = px.bar(
        df_errors,
        x='Issue Type',
        y='Count',
        text='Count',
        title=f"Error Distribution ({stats['rejected']} rejected samples)",
        hover_data=['Percentage'],
        color='Count',
        color_continuous_scale='Reds'
    )
    
    fig.update_traces(textposition='outside')
    fig.update_layout(
        showlegend=False,
        height=400,
        xaxis_title="Issue Type",
        yaxis_title="Number of Samples"
    )
    
    # Display chart with click handling
    selected_points = st.plotly_chart(
        fig,
        on_select="rerun",
        key="error_chart",
        use_container_width=True
    )
    
    # Handle bar click selection
    if selected_points and selected_points.selection and selected_points.selection.points:
        selected_issue = selected_points.selection.points[0].x
        
        st.divider()
        st.subheader(f"Samples with '{selected_issue}'")
        
        # Get filtered samples
        samples_with_issue = db.get_samples_by_issue(selected_issue)
        
        st.caption(f"Showing {len(samples_with_issue)} samples")
        
        # Display samples
        for sample, annotation in samples_with_issue:
            with st.expander(f"Sample: {sample.id}"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Prompt:**")
                    st.text(sample.prompt)
                    
                    if sample.metadata:
                        st.markdown("**Metadata:**")
                        st.json(sample.metadata)
                
                with col2:
                    st.markdown("**Response:**")
                    st.text(sample.response)
                    
                    if annotation.notes:
                        st.markdown("**Notes:**")
                        st.info(annotation.notes)
        
        # Export filtered samples
        st.divider()
        
        if st.button(f" Export these {len(samples_with_issue)} samples as CSV"):
            # Convert to dataframe
            export_data = []
            for sample, annotation in samples_with_issue:
                row = {
                    'id': sample.id,
                    'prompt': sample.prompt,
                    'response': sample.response,
                    'primary_issue': annotation.primary_issue,
                    'notes': annotation.notes,
                    **sample.metadata
                }
                export_data.append(row)
            
            df_export = pd.DataFrame(export_data)
            csv = df_export.to_csv(index=False)
            
            st.download_button(
                label="Download CSV",
                data=csv,
                file_name=f"rejected_{selected_issue}.csv",
                mime="text/csv",
                use_container_width=True
            )
    
    st.divider()
    
    # Metadata breakdown section
    st.subheader("Breakdown by Metadata")
    
    # Get all samples with annotations
    with db._get_connection() as conn:
        df_all = pd.read_sql_query("""
            SELECT s.metadata, a.is_acceptable
            FROM samples s
            JOIN annotations a ON s.id = a.sample_id
        """, conn)
    
    if df_all.empty:
        st.info("No annotated samples with metadata to analyze.")
        return
    
    # Parse metadata JSON with error handling
    def safe_json_loads(x):
        if not x or x.strip() == '':
            return {}
        try:
            return json.loads(x)
        except (json.JSONDecodeError, TypeError):
            return {}
    
    try:
        metadata_records = df_all['metadata'].apply(safe_json_loads)
        
        # Check if any metadata exists
        if not any(metadata_records.apply(bool)):
            st.info("No metadata fields found in samples.")
            return
        
        # Convert to dataframe
        metadata_df = pd.DataFrame(metadata_records.tolist())
        metadata_df['is_acceptable'] = df_all['is_acceptable'].values
        
        # Show breakdown for each metadata field
        for col in metadata_df.columns:
            if col != 'is_acceptable' and metadata_df[col].notna().any():
                st.markdown(f"**Acceptance rate by {col}:**")
                
                # Group and calculate acceptance rates
                breakdown = metadata_df.groupby(col)['is_acceptable'].agg(['sum', 'count'])
                breakdown['acceptance_rate'] = (breakdown['sum'] / breakdown['count'] * 100).round(1)
                breakdown = breakdown.sort_values('count', ascending=False)
                
                # Display as metrics in columns
                n_values = len(breakdown)
                cols = st.columns(min(n_values, 4))
                
                for i, (idx, row) in enumerate(breakdown.iterrows()):
                    col_idx = i % len(cols)
                    cols[col_idx].metric(
                        str(idx),
                        f"{row['acceptance_rate']:.1f}%",
                        f"{int(row['count'])} samples"
                    )
                
                st.divider()
    
    except Exception as e:
        st.error(f"Error parsing metadata: {str(e)}")
```

**Command to create:**

```bash
cat > ui/analysis_page.py << 'ANALYSISPAGEEOF'
[paste the complete code above]
ANALYSISPAGEEOF
```

#### Step 6.2: Update app.py to include analysis page

```bash
# Update the import and routing in app.py
```

**File: `app.py`** (updated)

```python
"""LabelBench - Main Streamlit application."""

import streamlit as st

from storage.database import Database
from ui.annotate_page import show_annotate_page
from ui.analysis_page import show_analysis_page
from ui.import_page import show_import_page


# Page configuration
st.set_page_config(
    page_title="LabelBench",
    page_icon="",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize database in session state
if 'db' not in st.session_state:
    st.session_state.db = Database()

# Sidebar navigation
with st.sidebar:
    st.title("LabelBench")
    st.caption("Annotation tool for LLM evaluation datasets")
    
    st.divider()
    
    page = st.radio(
        "Navigation",
        ["Annotate", " Error Analysis", " Import Data"],
        label_visibility="collapsed"
    )
    
    st.divider()
    
    # Quick stats in sidebar
    db = st.session_state.db
    total_samples = db.get_total_samples()
    stats = db.get_annotation_stats()
    
    st.metric("Total Samples", total_samples)
    st.metric("Annotated", stats['total_annotated'])
    
    if stats['total_annotated'] > 0:
        st.metric("Acceptance Rate", f"{stats['acceptance_rate']:.1f}%")

# Route to appropriate page
if page == " Annotate":
    show_annotate_page()
elif page == " Error Analysis":
    show_analysis_page()
elif page == " Import Data":
    show_import_page()
```

#### Step 6.3: Create summary report utility

**File: `utils/report.py`**

```python
"""Utilities for generating summary reports."""

from typing import Dict, Any


def generate_summary_report(stats: Dict[str, Any], error_dist: Dict[str, int]) -> str:
    """Generate a markdown summary report of annotation results.
    
    Args:
        stats: Dictionary with annotation statistics
        error_dist: Dictionary mapping error types to counts
        
    Returns:
        Markdown-formatted report string
    """
    report = f"""# LabelBench Annotation Report

## Summary Statistics

- **Total Samples Annotated:** {stats['total_annotated']}
- **Acceptance Rate:** {stats['acceptance_rate']:.1f}%
- **Accepted:** {stats['accepted']}
- **Rejected:** {stats['rejected']}

## Error Distribution

"""
    
    if error_dist:
        total_errors = sum(error_dist.values())
        for issue, count in sorted(error_dist.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_errors * 100)
            report += f"- **{issue}:** {count} ({percentage:.1f}%)\n"
    else:
        report += "*No errors captured*\n"
    
    report += """
## Recommendations

Based on the error distribution, consider:

"""
    
    if error_dist:
        top_issue, top_count = max(error_dist.items(), key=lambda x: x[1])
        top_percentage = (top_count / total_errors * 100)
        
        report += f"1. **Focus on '{top_issue}' errors** ({top_percentage:.1f}% of failures)\n"
        report += f"2. Review samples with this issue to identify patterns\n"
        report += f"3. Update system prompts or add validation logic to address this error type\n"
        
        if len(error_dist) > 1:
            report += f"4. Monitor other error types: {', '.join(list(error_dist.keys())[1:])}\n"
    else:
        report += "1. Continue monitoring for patterns as more samples are annotated\n"
    
    report += """
## Next Steps

- Export rejected samples for detailed review
- Use accepted samples as positive examples in prompts
- Build automated evaluations based on identified error patterns
"""
    
    return report
```

**Command to create:**

```bash
cat > utils/report.py << 'REPORTEOF'
[paste the complete code above]
REPORTEOF
```

#### Step 6.4: Add export functionality to analysis page

Update `ui/analysis_page.py` to add a "Generate Report" button at the bottom:

```python
# Add this at the end of show_analysis_page() function, before the final return

st.divider()
st.subheader("Export Report")

col1, col2 = st.columns(2)

with col1:
    from storage.import_export import export_rejected_csv
    
    if st.button("Export All Rejected Samples (CSV)", use_container_width=True):
        import tempfile
        
        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as tmp:
            count = export_rejected_csv(db, tmp.name)
            
            if count > 0:
                with open(tmp.name, 'r') as f:
                    csv_data = f.read()
                
                st.download_button(
                    label=f"Download {count} Rejected Samples",
                    data=csv_data,
                    file_name="rejected_samples.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            else:
                st.info("No rejected samples to export.")

with col2:
    from utils.report import generate_summary_report
    
    if st.button("Generate Summary Report", use_container_width=True):
        report = generate_summary_report(stats, error_dist)
        
        st.download_button(
            label="Download Report (Markdown)",
            data=report,
            file_name="labelbench_report.md",
            mime="text/markdown",
            use_container_width=True
        )
```

#### Step 6.5: Test the dashboard

```bash
# Run the app
uv run streamlit run app.py

# Manual testing checklist:
# 1. Import example_samples.csv if not already imported
# 2. Annotate at least 10 samples (mix of accept and reject)
# 3. Navigate to Error Analysis page
# 4. Verify stats display correctly
# 5. Click on an error bar - should show filtered samples
# 6. Verify export filtered samples works
# 7. Check metadata breakdown displays properly
# 8. Test both export buttons (rejected samples CSV and summary report)
```

#### Step 6.6: Commit error analysis dashboard

```bash
git add ui/analysis_page.py utils/report.py app.py
git commit -m "feat: Add interactive error analysis dashboard with Plotly"
git push origin main

# Create PR #4
# Title: "Error Analysis Dashboard: Interactive charts and export"
# Description: "Implements error analysis page with interactive Plotly bar chart, click-to-filter functionality, metadata breakdowns, sample detail view, and export capabilities (filtered samples CSV and summary report)"
```

---

### Phase 7: Documentation & Polish (PR #5)

#### Step 7.1: Create comprehensive README

**File: `README.md`**

```markdown
# LabelBench

A lightweight annotation tool for building golden LLM evaluation datasets.

## Features

-  **Import** prompt-response pairs from CSV/JSON
-  **Annotate** with binary Accept/Reject decisions and structured feedback
-  **Analyze** errors with interactive dashboard
-  **Breakdown** acceptance rates by metadata (model, task type, etc.)
-  **Local storage** with SQLite
-  **Export** rejected samples and summary reports

## Quick Start

```bash
# Clone the repository
git clone <your-repo-url>
cd labelbench

# Install dependencies with uv
uv sync

# Run the application
uv run streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`.

## Usage

### 1. Import Data

Navigate to ** Import Data** and upload a CSV or JSON file.

#### CSV Format

```csv
id,prompt,response,model,task_type
1,"What's your return policy?","30 days for full refund",gpt-4,support
2,"Do you ship to Canada?","Yes, free shipping!",gpt-3.5,support
```

**Required columns:** `id`, `prompt`, `response`  
**Optional columns:** Any additional columns are stored as metadata

#### JSON Format

```json
{
  "samples": [
    {
      "id": "1",
      "prompt": "What's your return policy?",
      "response": "30 days for full refund",
      "metadata": {
        "model": "gpt-4",
        "task_type": "support"
      }
    }
  ]
}
```

#### Try the Example Dataset

LabelBench includes an example dataset with 20 customer support samples:

```bash
# The file is located at: data/example_samples.csv
# Import it through the UI to get started immediately
```

### 2. Annotate Samples

Navigate to ** Annotate** to review samples:

1. Read the prompt and response
2. Click **Accept**  or **Reject** 
3. If rejecting, select a primary issue type and add notes
4. Use navigation buttons to move through samples

### 3. Analyze Errors

Navigate to ** Error Analysis** to view:

- Overall acceptance rate and statistics
- Interactive error distribution chart (click bars to filter)
- Detailed sample views with prompts, responses, and notes
- Breakdown by metadata (model, task type, etc.)

### 4. Export Results

From the Error Analysis page:

- **Export Rejected Samples:** Download CSV of all rejected samples with annotations
- **Generate Summary Report:** Download Markdown report with statistics and recommendations

## Project Structure

```
labelbench/
 app.py                    # Main Streamlit application
 models/                   # Pydantic data models
    sample.py
    annotation.py
 storage/                  # Database and I/O
    database.py
    import_export.py
 ui/                       # Streamlit pages
    annotate_page.py
    analysis_page.py
    import_page.py
 utils/                    # Utilities
    report.py
 data/                     # Example datasets
    example_samples.csv
 tests/                    # Unit tests
```

## Data Models

### Sample

Represents a prompt-response pair:

```python
{
  "id": str,              # Unique identifier
  "prompt": str,          # Input to the LLM
  "response": str,        # LLM's output
  "metadata": dict,       # Optional metadata (model, task_type, etc.)
  "imported_at": datetime
}
```

### Annotation

Represents human feedback:

```python
{
  "id": str,
  "sample_id": str,
  "is_acceptable": bool,                    # Binary decision
  "primary_issue": str | None,              # Reason for rejection
  "notes": str | None,                      # Detailed explanation
  "annotated_at": datetime
}
```

### Primary Issue Types

When rejecting a sample, choose from:

- **hallucination**: Response contains false information
- **factually_incorrect**: Response has factual errors
- **incomplete**: Response is missing key information
- **wrong_format**: Response doesn't follow expected format
- **off_topic**: Response doesn't address the prompt
- **inappropriate_tone**: Response has wrong tone/style
- **refusal**: Response refuses to answer when it shouldn't
- **other**: Other issues

## Development

### Running Tests

```bash
uv run pytest tests/ -v
```

### Adding New Features

1. Create a feature branch
2. Implement changes
3. Add tests
4. Submit pull request

## Database

LabelBench uses SQLite for local storage. The database file (`labelbench.db`) is created automatically in the project root.

To reset the database:

```bash
rm labelbench.db
uv run streamlit run app.py  # Will create a fresh database
```

## Tips for Effective Annotation

1. **Be consistent**: Use the same criteria for all samples
2. **Use notes**: Document patterns you notice in rejections
3. **Review periodically**: Check the Error Analysis dashboard to identify trends
4. **Export frequently**: Save rejected samples to prevent data loss

## Troubleshooting

### Import errors

- Verify CSV has required columns: `id`, `prompt`, `response`
- Check for special characters or encoding issues
- Try opening CSV in a text editor to verify format

### Samples not appearing

- Check that samples were imported successfully (Import Data page shows confirmation)
- Verify unannotated samples exist (sidebar shows "Annotated" count)

### Dashboard not showing data

- Ensure you've annotated at least one sample
- For error analysis, ensure you've rejected at least one sample
- Refresh the page if data doesn't update immediately

## License

MIT License - See LICENSE file for details

## Contributing

Contributions welcome! Please open an issue or pull request.

## Acknowledgments

Built with:
- [Streamlit](https://streamlit.io/) - Web UI framework
- [Pydantic](https://pydantic.dev/) - Data validation
- [Plotly](https://plotly.com/) - Interactive charts
- [Pandas](https://pandas.pydata.org/) - Data manipulation
```

**Command to create:**

```bash
cat > README.md << 'READMEEOF'
[paste the complete code above]
READMEEOF
```

#### Step 7.2: Take screenshots

```bash
# Manual steps:
# 1. Run uv run streamlit run app.py
# 2. Import example_samples.csv
# 3. Annotate a few samples (mix of accept/reject)
# 4. Take screenshots of:
#    - Import page with preview
#    - Annotation interface showing a sample
#    - Annotation interface with rejection form
#    - Error Analysis dashboard with chart
#    - Error Analysis with clicked bar showing filtered samples
# 5. Save screenshots in a docs/ or screenshots/ folder
# 6. Update README to include screenshot links

mkdir -p docs/screenshots
# Add screenshot paths to README under a new "Screenshots" section
```

#### Step 7.3: Final testing and bug fixes

```bash
# Complete end-to-end test:
# 1. Fresh database: rm labelbench.db
# 2. Start app: uv run streamlit run app.py
# 3. Import example dataset
# 4. Annotate all 20 samples
# 5. Review error analysis
# 6. Export rejected samples
# 7. Generate summary report
# 8. Verify all exports are valid

# Run full test suite
uv run pytest tests/ -v --cov=. --cov-report=html

# Check for any issues and fix
```

#### Step 7.4: Add license

**File: `LICENSE`**

```
MIT License

Copyright (c) 2025 [Your Name]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

**Command to create:**

```bash
cat > LICENSE << 'LICENSEEOF'
[paste the complete MIT license above]
LICENSEEOF
```

#### Step 7.5: Commit documentation

```bash
git add README.md LICENSE docs/
git commit -m "docs: Add comprehensive README, screenshots, and license"
git push origin main

# Create PR #5
# Title: "Documentation & Polish: README, screenshots, and final touches"
# Description: "Adds comprehensive README with usage instructions, screenshots of all major features, MIT license, and final bug fixes"
```

---

## Testing Strategy

### Unit Tests Coverage

```bash
# Run tests with coverage
uv run pytest tests/ -v --cov=. --cov-report=html --cov-report=term

# Target coverage: >80% for core modules
# - models/: 100% (simple Pydantic models)
# - storage/database.py: >90%
# - storage/import_export.py: >85%
# - UI pages: Manual testing (Streamlit doesn't unit test well)
```

### Manual Testing Checklist

**Import Flow:**
- [ ] Import valid CSV
- [ ] Import valid JSON
- [ ] Handle invalid file format
- [ ] Handle missing required columns
- [ ] Handle duplicate IDs
- [ ] Preview shows correct data
- [ ] Import confirmation message appears

**Annotation Flow:**
- [ ] Display sample with metadata
- [ ] Accept button saves annotation
- [ ] Reject button shows form
- [ ] Rejection form saves with issue and notes
- [ ] Rejection form can be cancelled
- [ ] Navigation: Previous, Next, Jump work correctly
- [ ] Progress bar updates
- [ ] Reaching end of samples reloads unannotated
- [ ] Sidebar stats update after annotation

**Error Analysis:**
- [ ] Stats display correctly
- [ ] Empty state messages show appropriately
- [ ] Bar chart renders with correct data
- [ ] Clicking bar filters samples
- [ ] Filtered samples show prompt, response, notes
- [ ] Export filtered samples produces valid CSV
- [ ] Metadata breakdown displays correctly
- [ ] Export all rejected samples works
- [ ] Summary report generates with correct data

---

## Git Workflow

### Branch Strategy

```bash
# Main branch: stable, deployable code
# Feature branches: one per PR

# For PR #1:
git checkout -b pr/1-foundation
# ... make changes ...
git push origin pr/1-foundation
# Create pull request on GitHub
# After merge, delete branch

# For PR #2:
git checkout main
git pull origin main
git checkout -b pr/2-import-export
# ... make changes ...
```

### Commit Message Format

```
<type>: <short description>

[optional body]

<type> options:
- feat: New feature
- fix: Bug fix
- docs: Documentation changes
- test: Test additions/changes
- refactor: Code restructuring
- chore: Maintenance tasks
```

**Examples:**

```
feat: Add Sample and Annotation Pydantic models

fix: Handle empty metadata in CSV import

docs: Add usage examples to README

test: Add unit tests for database CRUD operations
```

---

## CodeRabbit Setup

### Installation

1. Go to your GitHub repository
2. Navigate to Settings → GitHub Apps
3. Install CodeRabbit app
4. Grant access to the repository

### Configuration (Optional)

Create `.coderabbit.yaml` in project root:

```yaml
language: "en-US"
tone_instructions: "Be constructive and specific. Focus on code quality, test coverage, and potential edge cases."
reviews:
  auto_review: true
  request_changes_workflow: true
  high_level_summary: true
```

### Expected CodeRabbit Feedback

**PR #1 (Foundation):**
- Pydantic model validation
- Database schema design
- Index usage
- Error handling in CRUD operations
- Test coverage

**PR #2 (Import/Export):**
- CSV/JSON parsing edge cases
- File validation logic
- Pandas usage optimization
- Exception handling

**PR #3 (Annotation UI):**
- Streamlit state management
- Navigation logic
- Form validation
- UX flow

**PR #4 (Dashboard):**
- Plotly chart configuration
- SQL query optimization
- Aggregation logic
- Export functionality

**PR #5 (Documentation):**
- README completeness
- Code comments
- Docstring quality

---

## Common Pitfalls & Solutions

### Streamlit State Management

**Problem:** State resets unexpectedly on rerun

**Solution:**
```python
# Always use session_state for persistence
if 'key' not in st.session_state:
    st.session_state.key = initial_value

# Call st.rerun() explicitly when needed
```

### Plotly Click Events

**Problem:** `on_select` doesn't work as expected

**Solution:**
```python
# Ensure you're using st.plotly_chart with on_select="rerun"
selected = st.plotly_chart(fig, on_select="rerun", key="unique_key")

# Check for selection carefully
if selected and selected.selection and selected.selection.points:
    # Access point data
    point = selected.selection.points[0]
```

### Database Connections

**Problem:** Database locked errors

**Solution:**
```python
# Use context manager (recommended - automatic transaction management)
with db._get_connection() as conn:
    # ... queries ...
    # Auto-commits on success, rolls back on error, always closes connection
```

### CSV Export with Metadata

**Problem:** Metadata not expanding into columns

**Solution:**
```python
# Parse JSON metadata and expand
metadata_df = df['metadata'].apply(json.loads).apply(pd.Series)
df = pd.concat([df.drop('metadata', axis=1), metadata_df], axis=1)
```

---

## Performance Considerations

### Database

- SQLite is fine for <10K samples
- Indexes are created on frequently queried columns
- For >10K samples, consider PostgreSQL migration

### Streamlit

- Avoid loading all samples into memory
- Use database queries to filter
- Pagination for large result sets (future enhancement)

### File Uploads

- Streamlit handles files up to 200MB by default
- For larger files, increase limit in `.streamlit/config.toml`:

```toml
[server]
maxUploadSize = 500
```

---

## Extension Ideas (Post-v1)

### Multi-Annotator Support
- Add user authentication
- Track annotator_id per annotation
- Calculate inter-annotator agreement (Cohen's Kappa)

### Advanced Analysis
- Time-series charts (acceptance rate over time)
- Confusion matrix for error types
- Sample-level confidence scores

### Integrations
- Direct import from LangSmith, Helicone
- Export to lm-evaluation-harness format
- API for programmatic access

### UX Improvements
- Keyboard shortcuts (a for accept, r for reject)
- Bulk annotation mode
- Search and filter samples
- Sample difficulty rating

---

## Success Criteria

### Technical
- [ ] All 5 PRs merged successfully
- [ ] CodeRabbit provided meaningful reviews
- [ ] Test coverage >80% for core modules
- [ ] No critical bugs in main workflow

### Functional
- [ ] Can import 200 samples in <1 minute
- [ ] Can annotate 50 samples in <30 minutes
- [ ] Dashboard reveals actionable insights
- [ ] Exported data is immediately usable

### Documentation
- [ ] README is comprehensive
- [ ] Screenshots illustrate all major features
- [ ] Code is well-commented
- [ ] Installation steps are <5 commands

---

**End of Technical Specification**

---

## For AI Coding Tools: Execution Notes

### Claude Code

Execute this spec sequentially, section by section. After each phase, run tests and commit before moving to the next phase. The spec provides complete code blocks that can be written directly to files.

### Cursor

Use this spec as reference while developing. The architecture section helps with IDE suggestions. When implementing a function, refer to the corresponding section for complete implementation details.

### General

- All code blocks are production-ready
- File paths are explicit
- Commands are copy-paste ready
- Tests are included for verification
- Git workflow is fully documented

**Priority:** Error analysis is the most critical feature. If time is limited, ensure PRs #1-4 are rock-solid before PR #5.

```python
from pydantic import BaseModel, Field
from datetime import datetime
from typing import Literal, Optional

class Sample(BaseModel):
    """Represents a prompt-response pair to be annotated"""
    id: str = Field(..., description="Unique identifier for the sample")
    prompt: str = Field(..., description="Input to the LLM")
    response: str = Field(..., description="LLM's output")
    metadata: dict = Field(default_factory=dict, description="Flexible metadata (model, task_type, etc.)")
    imported_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

class Annotation(BaseModel):
    """Represents human feedback on a sample"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    sample_id: str = Field(..., description="Foreign key to Sample")
    annotator_id: str = Field(default="default", description="For future multi-annotator support")
    
    # Core judgment
    is_acceptable: bool = Field(..., description="Binary quality decision")
    
    # Structured feedback (only required if rejected)
    primary_issue: Optional[Literal[
        "hallucination",
        "factually_incorrect",
        "incomplete",
        "wrong_format",
        "off_topic",
        "inappropriate_tone",
        "refusal",
        "other"
    ]] = Field(None, description="Main reason for rejection")
    
    # Free-form notes
    notes: Optional[str] = Field(None, description="Detailed explanation")
    
    # Metadata
    annotated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
```

---

## Database Schema

### SQLite Tables

```sql
-- Samples table
CREATE TABLE samples (
    id TEXT PRIMARY KEY,
    prompt TEXT NOT NULL,
    response TEXT NOT NULL,
    metadata TEXT,  -- JSON-serialized dict
    imported_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_samples_imported ON samples(imported_at);

-- Annotations table
CREATE TABLE annotations (
    id TEXT PRIMARY KEY,
    sample_id TEXT NOT NULL,
    annotator_id TEXT DEFAULT 'default',
    is_acceptable BOOLEAN NOT NULL,
    primary_issue TEXT,
    notes TEXT,
    annotated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (sample_id) REFERENCES samples(id) ON DELETE CASCADE
);

CREATE INDEX idx_annotations_sample ON annotations(sample_id);
CREATE INDEX idx_annotations_acceptable ON annotations(is_acceptable);
CREATE INDEX idx_annotations_issue ON annotations(primary_issue);
```

### Database Operations (CRUD)

```python
import sqlite3
import json
from typing import List, Optional

class Database:
    def __init__(self, db_path: str = "labelbench.db"):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        """Create tables if they don't exist"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""CREATE TABLE IF NOT EXISTS samples (...)""")
        conn.execute("""CREATE TABLE IF NOT EXISTS annotations (...)""")
        conn.commit()
        conn.close()
    
    def insert_samples(self, samples: List[Sample]) -> int:
        """Insert samples, skip duplicates"""
        conn = sqlite3.connect(self.db_path)
        inserted = 0
        for sample in samples:
            try:
                conn.execute(
                    """INSERT INTO samples (id, prompt, response, metadata, imported_at) 
                       VALUES (?, ?, ?, ?, ?)""",
                    (sample.id, sample.prompt, sample.response, 
                     json.dumps(sample.metadata), sample.imported_at)
                )
                inserted += 1
            except sqlite3.IntegrityError:
                # Skip duplicate
                continue
        conn.commit()
        conn.close()
        return inserted
    
    def get_sample(self, sample_id: str) -> Optional[Sample]:
        """Get a single sample by ID"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.execute(
            "SELECT * FROM samples WHERE id = ?", (sample_id,)
        )
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return Sample(
                id=row['id'],
                prompt=row['prompt'],
                response=row['response'],
                metadata=json.loads(row['metadata']) if row['metadata'] else {},
                imported_at=row['imported_at']
            )
        return None
    
    def get_unannotated_samples(self) -> List[Sample]:
        """Get samples that haven't been annotated yet"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.execute("""
            SELECT s.* FROM samples s
            LEFT JOIN annotations a ON s.id = a.sample_id
            WHERE a.id IS NULL
            ORDER BY s.imported_at
        """)
        rows = cursor.fetchall()
        conn.close()
        
        return [Sample(
            id=row['id'],
            prompt=row['prompt'],
            response=row['response'],
            metadata=json.loads(row['metadata']) if row['metadata'] else {},
            imported_at=row['imported_at']
        ) for row in rows]
    
    def insert_annotation(self, annotation: Annotation):
        """Save an annotation"""
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            INSERT OR REPLACE INTO annotations 
            (id, sample_id, annotator_id, is_acceptable, primary_issue, notes, annotated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            annotation.id,
            annotation.sample_id,
            annotation.annotator_id,
            annotation.is_acceptable,
            annotation.primary_issue,
            annotation.notes,
            annotation.annotated_at
        ))
        conn.commit()
        conn.close()
    
    def get_annotation_stats(self) -> dict:
        """Get summary statistics"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute("""
            SELECT 
                COUNT(*) as total_annotated,
                SUM(CASE WHEN is_acceptable = 1 THEN 1 ELSE 0 END) as accepted,
                SUM(CASE WHEN is_acceptable = 0 THEN 1 ELSE 0 END) as rejected
            FROM annotations
        """)
        row = cursor.fetchone()
        conn.close()
        
        return {
            "total_annotated": row[0],
            "accepted": row[1],
            "rejected": row[2],
            "acceptance_rate": (row[1] / row[0] * 100) if row[0] > 0 else 0
        }
    
    def get_error_distribution(self) -> dict:
        """Get count of each error type"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute("""
            SELECT primary_issue, COUNT(*) as count
            FROM annotations
            WHERE is_acceptable = 0 AND primary_issue IS NOT NULL
            GROUP BY primary_issue
            ORDER BY count DESC
        """)
        rows = cursor.fetchall()
        conn.close()
        
        return {row[0]: row[1] for row in rows}
    
    def get_samples_by_issue(self, issue_type: str) -> List[tuple[Sample, Annotation]]:
        """Get all samples with a specific issue type"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.execute("""
            SELECT s.*, a.*
            FROM samples s
            JOIN annotations a ON s.id = a.sample_id
            WHERE a.primary_issue = ?
            ORDER BY a.annotated_at DESC
        """, (issue_type,))
        rows = cursor.fetchall()
        conn.close()
        
        results = []
        for row in rows:
            sample = Sample(
                id=row['id'],
                prompt=row['prompt'],
                response=row['response'],
                metadata=json.loads(row['metadata']) if row['metadata'] else {}
            )
            annotation = Annotation(
                id=row['id'],
                sample_id=row['sample_id'],
                is_acceptable=bool(row['is_acceptable']),
                primary_issue=row['primary_issue'],
                notes=row['notes']
            )
            results.append((sample, annotation))
        
        return results
```

---

## Project Structure

### Directory Layout

```
labelbench/
 app.py                      # Streamlit entry point & page router
 models/
    __init__.py
    sample.py               # Sample Pydantic model
    annotation.py           # Annotation Pydantic model
 storage/
    __init__.py
    database.py             # Database class with CRUD operations
    import_export.py        # CSV/JSON import and export utilities
 ui/
    __init__.py
    annotate_page.py        # Annotation interface
    analysis_page.py        # Error analysis dashboard
    import_page.py          # Import CSV/JSON page
 utils/
    __init__.py
    report.py               # Summary report generation
 data/
    example_samples.csv     # Demo dataset
    .gitkeep
 tests/
    __init__.py
    test_database.py
    test_import.py
    test_models.py
 .gitignore
 requirements.txt
 README.md
 labelbench.db               # SQLite database (gitignored)
```

### Create Directory Structure

```bash
# Create all directories
mkdir -p models storage ui utils data tests

# Create __init__.py files
touch models/__init__.py
touch storage/__init__.py
touch ui/__init__.py
touch utils/__init__.py
touch tests/__init__.py

# Create placeholder for data directory
touch data/.gitkeep

# Verify structure
tree -L 2
```

---

## Implementation Sequence

### Phase 1: Dependencies (PR #1 Setup)

## Streamlit Application Structure

### Main Entry Point (`app.py`)

```python
import streamlit as st
from ui.annotate_page import show_annotate_page
from ui.analysis_page import show_analysis_page
from ui.import_page import show_import_page

st.set_page_config(
    page_title="LabelBench",
    page_icon="",
    layout="wide"
)

# Initialize database connection in session state
if 'db' not in st.session_state:
    from storage.database import Database
    st.session_state.db = Database()

# Page navigation
page = st.sidebar.selectbox(
    "Navigation",
    ["Annotate", " Error Analysis", " Import Data"]
)

if page == " Annotate":
    show_annotate_page()
elif page == " Error Analysis":
    show_analysis_page()
elif page == " Import Data":
    show_import_page()
```

---

### Annotation Interface (`ui/annotate_page.py`)

**Key Requirements:**
- Display one sample at a time (prompt + response + metadata)
- Binary Accept/Reject buttons
- Conditional primary issue dropdown (only shown if rejected)
- Notes text area
- Navigation: Next, Previous, Jump to Sample #
- Progress indicator
- Save to database on button click

**UI Layout:**

```

  Sample 23 of 200                                23/200    

  ID: sample_42  |  model: gpt-4  |  task: customer_support  
                                                               
  Prompt:                                                      
    
   What's your return policy?                              
    
                                                               
  Response:                                                    
    
   You can return items within 30 days for a full         
   refund with proof of purchase.                          
    
                                                               
                             
     Accept         Reject                             
                             
                                                               
  [If Reject clicked:]                                         
  Primary Issue: [Dropdown                          ]        
                                                               
  Notes:                                                       
    
                                                           
    
                                                               
  [← Previous]              [Skip]             [Next →]       

```

**Implementation Pattern:**

```python
import streamlit as st
from models.sample import Sample
from models.annotation import Annotation

def show_annotate_page():
    st.title("Annotate Samples")
    
    db = st.session_state.db
    
    # Get unannotated samples
    if 'samples_to_annotate' not in st.session_state:
        st.session_state.samples_to_annotate = db.get_unannotated_samples()
        st.session_state.current_index = 0
    
    samples = st.session_state.samples_to_annotate
    
    if not samples:
        st.info("No samples to annotate. Import data to get started.")
        return
    
    # Current sample
    current_idx = st.session_state.current_index
    sample = samples[current_idx]
    
    # Progress
    st.progress((current_idx + 1) / len(samples))
    st.caption(f"Sample {current_idx + 1} of {len(samples)}")
    
    # Display metadata
    if sample.metadata:
        cols = st.columns(len(sample.metadata))
        for i, (key, value) in enumerate(sample.metadata.items()):
            cols[i].metric(key, value)
    
    # Display prompt and response
    st.subheader("Prompt")
    st.text_area("", value=sample.prompt, height=100, disabled=True, key="prompt_display")
    
    st.subheader("Response")
    st.text_area("", value=sample.response, height=150, disabled=True, key="response_display")
    
    st.divider()
    
    # Annotation controls
    col1, col2 = st.columns(2)
    
    with col1:
        accept = st.button("Accept", use_container_width=True, type="primary")
    with col2:
        reject = st.button("Reject", use_container_width=True, type="secondary")
    
    # Handle annotation
    if accept or reject:
        is_acceptable = accept
        
        primary_issue = None
        notes = None
        
        if reject:
            # Show feedback form
            with st.form("rejection_form"):
                primary_issue = st.selectbox(
                    "Primary Issue",
                    ["hallucination", "factually_incorrect", "incomplete", 
                     "wrong_format", "off_topic", "inappropriate_tone", 
                     "refusal", "other"]
                )
                notes = st.text_area("Notes (optional)")
                
                submitted = st.form_submit_button("Submit Annotation")
                
                if submitted:
                    annotation = Annotation(
                        sample_id=sample.id,
                        is_acceptable=False,
                        primary_issue=primary_issue,
                        notes=notes
                    )
                    db.insert_annotation(annotation)
                    
                    # Move to next sample
                    st.session_state.current_index += 1
                    st.rerun()
        else:
            # Accept - save immediately
            annotation = Annotation(
                sample_id=sample.id,
                is_acceptable=True
            )
            db.insert_annotation(annotation)
            
            # Move to next sample
            st.session_state.current_index += 1
            st.rerun()
    
    # Navigation
    st.divider()
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("← Previous") and current_idx > 0:
            st.session_state.current_index -= 1
            st.rerun()
    
    with col2:
        jump_to = st.number_input("Jump to sample", 1, len(samples), current_idx + 1)
        if st.button("Go"):
            st.session_state.current_index = jump_to - 1
            st.rerun()
    
    with col3:
        if st.button("Next →") and current_idx < len(samples) - 1:
            st.session_state.current_index += 1
            st.rerun()
```

---

### Error Analysis Dashboard (`ui/analysis_page.py`)

**Key Requirements:**
- Show overall stats (acceptance rate, counts)
- Interactive Plotly bar chart of error distribution
- Click bar → filter samples with that issue type
- Display filtered samples with prompt, response, notes
- Export filtered results as CSV
- Metadata breakdowns (acceptance rate by model, task type, etc.)

**Implementation Pattern:**

```python
import streamlit as st
import plotly.express as px
import pandas as pd

def show_analysis_page():
    st.title("Error Analysis")
    
    db = st.session_state.db
    
    # Get stats
    stats = db.get_annotation_stats()
    
    # Display overview
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Annotated", stats['total_annotated'])
    col2.metric("Acceptance Rate", f"{stats['acceptance_rate']:.1f}%")
    col3.metric("Rejected", stats['rejected'])
    
    if stats['rejected'] == 0:
        st.info("No rejected samples yet. Start annotating to see error analysis.")
        return
    
    st.divider()
    
    # Error distribution chart
    st.subheader("What's Breaking?")
    st.caption("Click on a bar to see all samples with that issue")
    
    error_dist = db.get_error_distribution()
    
    if not error_dist:
        st.warning("No error types captured yet.")
        return
    
    # Create dataframe for plotting
    df = pd.DataFrame({
        'Issue Type': list(error_dist.keys()),
        'Count': list(error_dist.values())
    })
    
    # Calculate percentages
    df['Percentage'] = (df['Count'] / df['Count'].sum() * 100).round(1)
    df = df.sort_values('Count', ascending=False)
    
    # Interactive Plotly chart
    fig = px.bar(
        df, 
        x='Issue Type', 
        y='Count',
        text='Count',
        title=f"Error Distribution ({stats['rejected']} rejected samples)",
        hover_data=['Percentage']
    )
    
    fig.update_traces(textposition='outside')
    fig.update_layout(showlegend=False, height=400)
    
    # Display with click handling
    selected = st.plotly_chart(fig, on_select="rerun", key="error_chart", use_container_width=True)
    
    # Handle selection
    if selected and selected.selection and selected.selection.points:
        selected_issue = selected.selection.points[0].x
        
        st.divider()
        st.subheader(f"Samples with '{selected_issue}'")
        
        # Get filtered samples
        samples_with_issue = db.get_samples_by_issue(selected_issue)
        
        st.caption(f"Showing {len(samples_with_issue)} samples")
        
        # Display samples
        for sample, annotation in samples_with_issue:
            with st.expander(f"Sample: {sample.id}"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("**Prompt:**")
                    st.text(sample.prompt)
                
                with col2:
                    st.markdown("**Response:**")
                    st.text(sample.response)
                
                if annotation.notes:
                    st.markdown("**Notes:**")
                    st.info(annotation.notes)
                
                # Display metadata
                if sample.metadata:
                    st.markdown("**Metadata:**")
                    st.json(sample.metadata)
        
        # Export button
        if st.button(f"Export these {len(samples_with_issue)} samples as CSV"):
            # Convert to dataframe
            export_data = []
            for sample, annotation in samples_with_issue:
                row = {
                    'id': sample.id,
                    'prompt': sample.prompt,
                    'response': sample.response,
                    'primary_issue': annotation.primary_issue,
                    'notes': annotation.notes,
                    **sample.metadata
                }
                export_data.append(row)
            
            df_export = pd.DataFrame(export_data)
            csv = df_export.to_csv(index=False)
            
            st.download_button(
                label="Download CSV",
                data=csv,
                file_name=f"rejected_{selected_issue}.csv",
                mime="text/csv"
            )
    
    st.divider()
    
    # Metadata breakdowns
    st.subheader("Breakdown by Metadata")
    
    # Get all samples with annotations and metadata
    with db._get_connection() as conn:
        df_all = pd.read_sql_query("""
            SELECT s.metadata, a.is_acceptable
            FROM samples s
            JOIN annotations a ON s.id = a.sample_id
        """, conn)
    
    if not df_all.empty:
        # Parse metadata with error handling
        def safe_json_loads(x):
            if not x or x.strip() == '':
                return {}
            try:
                return json.loads(x)
            except (json.JSONDecodeError, TypeError):
                return {}
        
        df_all['metadata'] = df_all['metadata'].apply(safe_json_loads)
        
        # Extract metadata fields
        metadata_df = pd.json_normalize(df_all['metadata'])
        metadata_df['is_acceptable'] = df_all['is_acceptable']
        
        # Show breakdown for each metadata field
        for col in metadata_df.columns:
            if col != 'is_acceptable':
                st.markdown(f"**Acceptance rate by {col}:**")
                breakdown = metadata_df.groupby(col)['is_acceptable'].agg(['sum', 'count'])
                breakdown['acceptance_rate'] = (breakdown['sum'] / breakdown['count'] * 100).round(1)
                
                for idx, row in breakdown.iterrows():
                    st.text(f"  {idx}: {row['acceptance_rate']:.1f}% ({int(row['count'])} samples)")
```

---

## Import/Export Functionality

### Import (`storage/import_export.py`)

```python
import pandas as pd
import json
from typing import List
from models.sample import Sample

def import_csv(file_path: str) -> List[Sample]:
    """Import samples from CSV"""
    df = pd.read_csv(file_path)
    
    # Validate required columns
    required = ['id', 'prompt', 'response']
    missing = set(required) - set(df.columns)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    
    # Convert to Sample objects
    samples = []
    for _, row in df.iterrows():
        # Separate required fields from metadata
        metadata = {k: v for k, v in row.items() 
                   if k not in required and pd.notna(v)}
        
        sample = Sample(
            id=str(row['id']),
            prompt=str(row['prompt']),
            response=str(row['response']),
            metadata=metadata
        )
        samples.append(sample)
    
    return samples

def import_json(file_path: str) -> List[Sample]:
    """Import samples from JSON"""
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    if 'samples' not in data:
        raise ValueError("JSON must contain 'samples' key")
    
    samples = []
    for item in data['samples']:
        if 'id' not in item or 'prompt' not in item or 'response' not in item:
            raise ValueError("Each sample must have id, prompt, and response")
        
        sample = Sample(
            id=item['id'],
            prompt=item['prompt'],
            response=item['response'],
            metadata=item.get('metadata', {})
        )
        samples.append(sample)
    
    return samples

def export_rejected_csv(db, output_path: str):
    """Export all rejected samples with annotations to CSV"""
    with db._get_connection() as conn:
        df = pd.read_sql_query("""
            SELECT 
                s.id,
                s.prompt,
                s.response,
                s.metadata,
                a.primary_issue,
                a.notes,
                a.annotated_at
            FROM samples s
            JOIN annotations a ON s.id = a.sample_id
            WHERE a.is_acceptable = 0
            ORDER BY a.annotated_at DESC
        """, conn)
    
    # Parse metadata into separate columns with error handling
    if not df.empty:
        def safe_json_loads(x):
            if not x or x.strip() == '':
                return {}
            try:
                return json.loads(x)
            except (json.JSONDecodeError, TypeError):
                return {}
        
        metadata_df = df['metadata'].apply(safe_json_loads).apply(pd.Series)
        df = pd.concat([df.drop('metadata', axis=1), metadata_df], axis=1)
    
    df.to_csv(output_path, index=False)
    return len(df)
```

### Summary Report Generation (`utils/report.py`)

```python
def generate_summary_report(db) -> str:
    """Generate a markdown summary report"""
    stats = db.get_annotation_stats()
    error_dist = db.get_error_distribution()
    
    report = f"""# LabelBench Annotation Report

## Summary Statistics

- **Total Samples Annotated:** {stats['total_annotated']}
- **Acceptance Rate:** {stats['acceptance_rate']:.1f}%
- **Accepted:** {stats['accepted']}
- **Rejected:** {stats['rejected']}

## Error Distribution

"""
    
    if error_dist:
        total_errors = sum(error_dist.values())
        for issue, count in sorted(error_dist.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / total_errors * 100)
            report += f"- **{issue}:** {count} ({percentage:.1f}%)\n"
    else:
        report += "*No errors captured*\n"
    
    report += """
## Recommendations

Based on the error distribution, consider:

"""
    
    if error_dist:
        top_issue = max(error_dist.items(), key=lambda x: x[1])[0]
        report += f"1. Focus on addressing '{top_issue}' errors (most common)\n"
        report += f"2. Review samples with this issue to identify patterns\n"
        report += f"3. Update system prompts or add validation logic\n"
    
    return report
```

---

## Requirements (`requirements.txt`)

```
streamlit>=1.31.0
pandas>=2.0.0
plotly>=5.18.0
pydantic>=2.5.0
```

---

## Example Dataset (`data/example_samples.csv`)

Create 20-30 diverse samples with intentional good and bad examples:

```csv
id,prompt,response,model,task_type
1,"What's your return policy?","You can return items within 30 days for a full refund.",gpt-4,customer_support
2,"Do you ship to Canada?","Yes, we offer free shipping to all of Canada!",gpt-3.5,customer_support
3,"What are your business hours?","We're open Monday through Friday, 9 AM to 5 PM EST.",gpt-4,customer_support
4,"Can I return opened products?","All products can be returned even if opened.",gpt-3.5,customer_support
5,"What's your refund policy?","Refunds are processed within 30 days.",gpt-4,customer_support
```

Include examples of:
- Correct responses (should be accepted)
- Hallucinations (wrong factual info)
- Incomplete responses (missing key details)
- Wrong format (not answering the question)
- Off-topic responses

---

## Testing Strategy

### Unit Tests

```python
# tests/test_database.py
import pytest
from storage.database import Database
from models.sample import Sample
from models.annotation import Annotation

def test_insert_and_retrieve_sample():
    db = Database(":memory:")  # In-memory database for testing
    
    sample = Sample(
        id="test_1",
        prompt="Test prompt",
        response="Test response",
        metadata={"model": "gpt-4"}
    )
    
    db.insert_samples([sample])
    retrieved = db.get_sample("test_1")
    
    assert retrieved is not None
    assert retrieved.prompt == "Test prompt"
    assert retrieved.metadata["model"] == "gpt-4"

def test_annotation_stats():
    db = Database(":memory:")
    
    # Insert samples and annotations
    samples = [Sample(id=f"s{i}", prompt="p", response="r") for i in range(10)]
    db.insert_samples(samples)
    
    # Annotate: 7 accepted, 3 rejected
    for i in range(7):
        db.insert_annotation(Annotation(sample_id=f"s{i}", is_acceptable=True))
    for i in range(7, 10):
        db.insert_annotation(Annotation(
            sample_id=f"s{i}", 
            is_acceptable=False,
            primary_issue="hallucination"
        ))
    
    stats = db.get_annotation_stats()
    assert stats['total_annotated'] == 10
    assert stats['accepted'] == 7
    assert stats['acceptance_rate'] == 70.0
```

---

## Git & PR Strategy

### PR Breakdown

| PR # | Title | Description | Files Changed |
|------|-------|-------------|---------------|
| 1 | Foundation: Data models and database | Pydantic models, SQLite schema, CRUD operations | `models/`, `storage/database.py`, `requirements.txt` |
| 2 | Import/Export functionality | CSV/JSON import, export functions, example dataset | `storage/import_export.py`, `data/example_samples.csv`, `ui/import_page.py` |
| 3 | Annotation interface | Streamlit UI for reviewing and annotating samples | `ui/annotate_page.py`, `app.py` |
| 4 | Error analysis dashboard | Interactive Plotly charts, filtering, metadata breakdowns | `ui/analysis_page.py`, `utils/report.py` |
| 5 | Documentation and polish | README, screenshots, bug fixes | `README.md`, bug fixes across codebase |

### Commit Message Format

```
feat: Add Sample and Annotation Pydantic models
fix: Handle duplicate sample IDs on import
docs: Add usage examples to README
refactor: Extract database queries to separate methods
test: Add unit tests for annotation stats
```

---

## README Structure

```markdown
# LabelBench

A lightweight annotation tool for building golden LLM evaluation datasets.

## Features

-  Import prompt-response pairs from CSV/JSON
-  Streamlined annotation with binary Accept/Reject decisions
-  Interactive error analysis dashboard
-  Automated breakdown by metadata (model, task type, etc.)
-  Local SQLite storage
-  Export rejected samples for downstream evaluation

## Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run the app
uv run streamlit run app.py

# Import example data and start annotating!
```

## Usage

1. **Import Data:** Upload CSV/JSON with `id`, `prompt`, `response` columns
2. **Annotate:** Review samples and make Accept/Reject decisions
3. **Analyze:** View error distribution and identify patterns
4. **Export:** Download rejected samples for fixing or evaluation

## Data Format

### CSV
```csv
id,prompt,response,model,task_type
1,"What's your return policy?","30 days for full refund",gpt-4,support
```

### JSON
```json
{
  "samples": [
    {
      "id": "1",
      "prompt": "What's your return policy?",
      "response": "30 days for full refund",
      "metadata": {"model": "gpt-4", "task_type": "support"}
    }
  ]
}
```

## Screenshots

[Add screenshots here]

## Development

```bash
# Run tests
pytest

# Format code
black .

# Type checking
mypy .
```

## License

MIT
```

---

## Implementation Checklist

### PR #1: Foundation
- [ ] Create project structure
- [ ] Define Pydantic models (Sample, Annotation)
- [ ] Implement Database class with SQLite
- [ ] Write CRUD operations
- [ ] Add basic unit tests
- [ ] Create pyproject.toml
- [ ] Submit PR #1

### PR #2: Import/Export
- [ ] Implement CSV import with validation
- [ ] Implement JSON import with validation
- [ ] Create example dataset (20-30 samples)
- [ ] Implement export functions (rejected samples, full dataset)
- [ ] Build import UI page in Streamlit
- [ ] Test import with example data
- [ ] Submit PR #2

### PR #3: Annotation UI
- [ ] Build annotation page layout
- [ ] Implement binary Accept/Reject buttons
- [ ] Add conditional primary issue dropdown
- [ ] Add notes text area
- [ ] Implement navigation (next, previous, jump)
- [ ] Add progress indicator
- [ ] Connect to database (save annotations)
- [ ] Test full annotation workflow
- [ ] Submit PR #3

### PR #4: Error Analysis Dashboard
- [ ] Implement aggregation queries (stats, error distribution)
- [ ] Build overview section (acceptance rate, counts)
- [ ] Create interactive Plotly bar chart
- [ ] Implement click-to-filter functionality
- [ ] Build sample detail view (expandable cards)
- [ ] Add metadata breakdown section
- [ ] Implement filtered export from dashboard
- [ ] Test all interactive features
- [ ] Submit PR #4

### PR #5: Polish & Documentation
- [ ] Write comprehensive README
- [ ] Add screenshots
- [ ] Implement summary report generation
- [ ] Fix bugs from end-to-end testing
- [ ] Add .gitignore for labelbench.db
- [ ] Code cleanup and comments
- [ ] Final testing
- [ ] Submit PR #5

### Buffer: CodeRabbit Review & Presentation
- [ ] Address CodeRabbit feedback
- [ ] Iterate on PRs based on reviews
- [ ] Prepare presentation slides
- [ ] Practice demo
- [ ] Document lessons learned

---

## Key Design Principles

1. **Error analysis is the priority:** The dashboard must make it trivially easy to identify patterns and drill down into specific failures.

2. **Binary decisions for clarity:** Accept/Reject forces clear judgments; structured feedback captures nuance.

3. **Local-first:** SQLite keeps things simple and private. Cloud is a future enhancement.

4. **Metadata flexibility:** Users can include any metadata fields; the tool adapts dynamically.

5. **Demo-friendly:** Include example data, one-command startup, clear README.

6. **Extensible:** Design for single-annotator now, but structure supports multi-annotator later.

---

## Common Pitfalls to Avoid

1. **Streamlit state management:** Use `st.session_state` carefully. Buttons trigger reruns, so store data persistently.

2. **Plotly click events:** `on_select` can be finicky. Test thoroughly and have fallback to non-interactive charts if needed.

3. **Database connections:** Always close connections. Consider context managers.

4. **Empty states:** Handle cases where there's no data imported, no annotations yet, etc.

5. **Metadata parsing:** Not all samples will have same metadata fields. Handle missing keys gracefully.

---

## Success Metrics

- Can import 200 samples and annotate 50 in <30 minutes
- Dashboard reveals actionable insights (e.g., "40% of errors are hallucinations")
- Exported rejected samples are immediately usable for fixing prompts
- Project runs with <5 commands after clone
- CodeRabbit provides meaningful feedback on all 5 PRs

---

**End of Technical Specification**
